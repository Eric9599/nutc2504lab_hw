import pdfplumber
from llm_guard.input_scanners import PromptInjection
from llm_guard.input_scanners.prompt_injection import MatchType
from docx import Document
from PIL import Image
import torch
import pandas as pd
import os
import uuid
import requests
import base64
import gc
from typing import List
from tqdm import tqdm
from openai import OpenAI
from qdrant_client import QdrantClient, models
from langchain_core.embeddings import Embeddings
from langchain_core.documents import Document as LCDocument
from langchain_text_splitters import RecursiveCharacterTextSplitter
from transformers import AutoTokenizer, AutoModelForCausalLM
from io import BytesIO

# ================= é…ç½®è¨­å®š =================
FILE_PATH = "HW"
FILE_LIST = ["1.pdf", "2.pdf", "3.pdf", "4.png", "5.docx"]
EMBEDDING_API_URL = "http://ws-04.wade0426.me/embed"
QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "rag_homework_day7_api"

# ä¸»è¦ LLM
LLM_BASE_URL = "https://ws-03.wade0426.me/v1"
LLM_API_KEY = "day7hw"
LLM_MODEL_NAME = "/models/Qwen3-30B-A3B-Instruct-2507-FP8"

# OLM OCR æ¨¡å‹
OLM_API_URL = "https://6c1f-163-17-132-191.ngrok-free.app/v1/chat/completions"
OLM_API_KEY = "day7hw"
OLM_MODEL_NAME = "allenai/olmOCR-2-7B-1025-FP8"

RERANKER_MODEL_PATH = os.path.expanduser("../day6/Qwen3-Reranker-0.6B")
PREDICT_INPUT = "HW/questions.csv"
PREDICT_OUTPUT = "HW/output.csv"
GROUND_TRUTH_OUTPUT = "HW/ground_truth.csv"

client = QdrantClient(url=QDRANT_URL)

if torch.cuda.is_available():
    device_obj = torch.device("cuda")
elif torch.backends.mps.is_available():
    device_obj = torch.device("mps")
else:
    device_obj = torch.device("cpu")
print(f"* Device: {device_obj}")


# ================= é¡åˆ¥èˆ‡å‡½å¼å®šç¾© =================

class CustomAPIEmbeddings(Embeddings):
    def __init__(self, api_url):
        self.api_url = api_url

    def _call_api(self, texts: List[str]) -> List[List[float]]:
        data = {"texts": texts, "normalize": True, "batch_size": 32}
        try:
            response = requests.post(self.api_url, json=data, timeout=60)
            if response.status_code == 200:
                return response.json().get('embeddings', [])
            else:
                print(f"âŒ API Error Code: {response.status_code}")
                return []
        except Exception as e:
            print(f"âŒ API Exception: {e}")
            return []

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._call_api(texts)

    def embed_query(self, text: str) -> List[float]:
        results = self._call_api([text])
        return results[0] if results else []


class SimpleLLMClient:
    def __init__(self, base_url, model_name, api_key):
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        self.model_name = model_name

    def generate(self, prompt: str) -> str:
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"LLM Error: {e}")
            return "Error generating response."


# åˆå§‹åŒ– Embedding èˆ‡ LLM
print(f"* Initial Embedding API ({EMBEDDING_API_URL})...")
embedding_model = CustomAPIEmbeddings(EMBEDDING_API_URL)
try:
    test_vec = embedding_model.embed_query("æ¸¬è©¦")
    if test_vec:
        EMBED_DIM = len(test_vec)
        print(f"âœ… API é€£ç·šæˆåŠŸï¼å‘é‡ç¶­åº¦: {EMBED_DIM}")
    else:
        raise ValueError("API å›å‚³ç‚ºç©º")
except Exception as e:
    print(f"âŒ API æ¸¬è©¦å¤±æ•—: {e}")
    exit()

print("* åˆå§‹åŒ– LLM Client...")
llm_client = SimpleLLMClient(LLM_BASE_URL, LLM_MODEL_NAME, LLM_API_KEY)


# --- OLM OCR åŠŸèƒ½ ---

def encode_image_to_base64(image_path, max_size=1024):
    """
    å°‡åœ–ç‰‡ç¸®å°å¾Œè½‰æ›ç‚º Base64 å­—ä¸²ï¼Œé¿å… Token è¶…éæ¨¡å‹ä¸Šé™ã€‚
    """
    with Image.open(image_path) as img:
        if max(img.size) > max_size:
            ratio = max_size / max(img.size)
            new_size = (int(img.width * ratio), int(img.height * ratio))
            img = img.resize(new_size, Image.Resampling.LANCZOS)
            print(f"   (åœ–ç‰‡éå¤§ï¼Œå·²ç¸®å°è‡³: {new_size})")

        buffered = BytesIO()
        img.save(buffered, format="PNG")
        return base64.b64encode(buffered.getvalue()).decode('utf-8')


def call_olm_ocr_api(image_path):
    """å‘¼å« OLM API é€²è¡Œåœ–ç‰‡è¾¨è­˜"""
    base64_image = encode_image_to_base64(image_path)

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {OLM_API_KEY}"
    }

    payload = {
        "model": OLM_MODEL_NAME,
        "messages": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Convert this image to markdown text. Preserve tables and formatting."
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{base64_image}"
                        }
                    }
                ]
            }
        ],
        "max_tokens": 4096,
        "temperature": 0.1
    }

    try:
        response = requests.post(OLM_API_URL, headers=headers, json=payload, timeout=120)
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            return content
        else:
            print(f"âŒ OLM API Error: {response.status_code} - {response.text}")
            return ""
    except Exception as e:
        print(f"âŒ OLM Connection Error: {e}")
        return ""


def process_idp_files(file_path, file_list):
    """è®€å–ä¸¦è™•ç†æ‰€æœ‰æª”æ¡ˆ (PDF æ··åˆæ¨¡å¼ + DOCX è¡¨æ ¼æ”¯æ´)"""
    docs_content = {}
    print("--- é–‹å§‹ IDP è™•ç† ---")

    for file_name in file_list:
        full_path = os.path.join(file_path, file_name)
        if not os.path.exists(full_path):
            print(f"* æ‰¾ä¸åˆ°æª”æ¡ˆ: {file_name}")
            continue

        print(f"æ­£åœ¨è™•ç†: {file_name}...")
        extracted_text = ""

        # === 1. PDF è™•ç† ===
        if file_name.endswith(".pdf"):
            try:
                with pdfplumber.open(full_path) as pdf:
                    for i, page in enumerate(pdf.pages):
                        text = page.extract_text()

                        # åˆ¤æ–·æ˜¯å¦ç‚ºæƒææª”
                        if not text or len(text.strip()) < 10:
                            print(f"   âš ï¸ ç¬¬ {i + 1} é ç–‘ä¼¼ç‚ºæƒææª”ï¼Œè½‰ç‚ºåœ–ç‰‡é€²è¡Œ OLM OCR...")
                            im = page.to_image(resolution=300).original
                            temp_img_path = f"temp_page_{i}.png"
                            im.save(temp_img_path)

                            ocr_text = call_olm_ocr_api(temp_img_path)
                            if ocr_text: extracted_text += ocr_text + "\n"

                            if os.path.exists(temp_img_path): os.remove(temp_img_path)
                        else:
                            extracted_text += text + "\n"
                            # æå–è¡¨æ ¼
                            tables = page.extract_tables()
                            for table in tables:
                                table_str = ""
                                for row in table:
                                    cleaned_row = [str(cell).strip() if cell is not None else "" for cell in row]
                                    if any(cleaned_row):
                                        table_str += " | ".join(cleaned_row) + "\n"
                                if table_str:
                                    extracted_text += "\n[è¡¨æ ¼]\n" + table_str + "\n"
                print(f"âœ… [{file_name}] PDF è™•ç†å®Œæˆ")
            except Exception as e:
                print(f"âŒ PDF éŒ¯èª¤: {e}")

        # === 2. PNG è™•ç† ===
        elif file_name.endswith(".png"):
            try:
                print(f"â³ æ­£åœ¨å‘¼å« OLM æ¨¡å‹è™•ç†åœ–ç‰‡ ({file_name})...")
                olm_text = call_olm_ocr_api(full_path)
                if olm_text:
                    extracted_text = olm_text
                    print(f"âœ… [{file_name}] OLM è¾¨è­˜æˆåŠŸ")
                else:
                    print(f"âš ï¸ [{file_name}] OLM æœªå›å‚³å…§å®¹")
            except Exception as e:
                print(f"âŒ OLM è™•ç†å¤±æ•— {file_name}: {e}")

        # === 3. DOCX è™•ç† (æ”¯æ´è¡¨æ ¼) ===
        elif file_name.endswith(".docx"):
            try:
                doc = Document(full_path)
                # æ®µè½
                for para in doc.paragraphs:
                    extracted_text += para.text + "\n"
                # è¡¨æ ¼
                for table in doc.tables:
                    table_str = ""
                    for row in table.rows:
                        row_cells = [cell.text.strip() for cell in row.cells]
                        if any(row_cells):
                            table_str += " | ".join(row_cells) + "\n"
                    if table_str:
                        extracted_text += "\n[è¡¨æ ¼]\n" + table_str + "\n"

                print(f"âœ… [{file_name}] Word è®€å–æˆåŠŸ (å«è¡¨æ ¼)")
            except Exception as e:
                print(f"âŒ Word éŒ¯èª¤: {e}")

        if extracted_text.strip():
            docs_content[file_name] = extracted_text
        else:
            print(f"âš ï¸ {file_name} å…§å®¹ç‚ºç©ºï¼Œè·³éã€‚")

    return docs_content


def scan_chunks_for_injection(split_docs):
    """
    ã€é—œéµä¿®æ”¹ã€‘é‡å°åˆ‡åˆ†å¾Œçš„ Chunk é€²è¡Œæƒæ
    é€™æ¨£å¯ä»¥é¿å…ï¼š
    1. å…§å®¹éé•·è¢«æˆªæ–·
    2. æƒ¡æ„ç‰¹å¾µè¢«å¤§é‡æ­£å¸¸æ–‡å­—ç¨€é‡‹
    """
    print("--- åŸ·è¡Œå®‰å…¨æ€§æƒæ (Chunk Level) ---")
    # å°æ–¼ Chunkï¼Œ0.5 çš„é–¾å€¼é€šå¸¸è¶³å¤ ï¼Œå› ç‚ºæ¿ƒåº¦è®Šé«˜äº†
    scanner = PromptInjection(threshold=0.5, match_type=MatchType.FULL)

    final_docs = []

    for doc in tqdm(split_docs, desc="æƒæ Chunks"):
        text_content = doc.page_content
        source = doc.metadata.get("source", "unknown")

        _, is_valid, score = scanner.scan(text_content)

        if not is_valid:
            print(f"âš ï¸ [è­¦å‘Š] æª”æ¡ˆ {source} çš„æŸå€‹å€å¡Šç–‘ä¼¼æƒ¡æ„ (Score: {score})")
            # é€™è£¡æ±ºå®šã€Œæ”¾è¡Œã€ä»¥ç¢ºä¿ä½œæ¥­èƒ½å›ç­”å•é¡Œï¼Œä½†å¯¦å‹™ä¸Šé€šå¸¸æœƒä¸Ÿæ£„
            final_docs.append(doc)
        else:
            final_docs.append(doc)

    return final_docs


def init_qdrant_collection(documents):
    """å¯«å…¥å‘é‡è³‡æ–™åº«"""
    if not documents:
        print("âš ï¸ ç„¡æ–‡ä»¶å¯å¯«å…¥")
        return

    print(f"ğŸ”„ é‡ç½®é›†åˆ {COLLECTION_NAME}...")
    try:
        client.delete_collection(COLLECTION_NAME)
    except:
        pass

    client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config={"dense": models.VectorParams(distance=models.Distance.COSINE, size=EMBED_DIM)},
        sparse_vectors_config={"sparse": models.SparseVectorParams(modifier=models.Modifier.IDF)},
    )

    texts_to_embed = [doc.page_content for doc in documents]
    print(f"â³ è¨ˆç®— {len(texts_to_embed)} ç­† Embeddings...")

    doc_embeddings = embedding_model.embed_documents(texts_to_embed)
    if len(doc_embeddings) != len(documents):
        print("âŒ Embedding æ•¸é‡ä¸ç¬¦")
        return

    points = []
    for doc, embedding in zip(documents, doc_embeddings):
        points.append(models.PointStruct(
            id=uuid.uuid4().hex,
            vector={
                "dense": embedding,
                "sparse": models.Document(text=doc.page_content, model="Qdrant/bm25"),
            },
            payload={
                "text": doc.page_content,
                "source": doc.metadata.get("source", "unknown")
            },
        ))

    batch_size = 50
    for i in tqdm(range(0, len(points), batch_size), desc="å¯«å…¥ Qdrant"):
        client.upsert(collection_name=COLLECTION_NAME, points=points[i: i + batch_size])
    print("âœ… è³‡æ–™å¯«å…¥å®Œæˆ")


# --- Reranker ç›¸é—œ ---
print("* è¼‰å…¥ Reranker æ¨¡å‹...")
reranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL_PATH, local_files_only=True, trust_remote_code=True)
reranker_model = AutoModelForCausalLM.from_pretrained(RERANKER_MODEL_PATH, local_files_only=True,
                                                      trust_remote_code=True).to(device_obj).eval()

token_false_id = reranker_tokenizer.convert_tokens_to_ids("no")
token_true_id = reranker_tokenizer.convert_tokens_to_ids("yes")
prefix = "<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\n<|im_start|>user\n"
suffix = "<|im_end|>\n<|im_start|>assistant\n"


def compute_rerank_scores(pairs, batch_size=4):
    all_scores = []
    for i in range(0, len(pairs), batch_size):
        batch_pairs = pairs[i: i + batch_size]
        processed_inputs = []
        for pair in batch_pairs:
            text = f"{prefix}{pair}{suffix}"
            processed_inputs.append(text)

        inputs = reranker_tokenizer(processed_inputs, padding=True, truncation=True, return_tensors="pt",
                                    max_length=1024).to(device_obj)

        with torch.no_grad():
            logits = reranker_model(**inputs).logits[:, -1, :]
            scores = logits[:, token_true_id].exp().tolist()
            all_scores.extend(scores)

        del inputs, logits
        torch.cuda.empty_cache() if torch.cuda.is_available() else None

    return all_scores


def rerank_documents(query, documents):
    if not documents: return []
    formatted_pairs = [f"<Instruct>: æ ¹æ“šæŸ¥è©¢æª¢ç´¢ç›¸é—œæ–‡ä»¶\n<Query>: {query}\n<Document>: {doc['text']}" for doc in
                       documents]
    scores = compute_rerank_scores(formatted_pairs)
    doc_scores = list(zip(documents, scores))
    doc_scores.sort(key=lambda x: x[1], reverse=True)
    return doc_scores


def hybrid_search_with_rerank(query: str, initial_limit=20, final_limit=3):
    query_vec = embedding_model.embed_query(query)
    try:
        response = client.query_points(
            collection_name=COLLECTION_NAME,
            prefetch=[
                models.Prefetch(query=models.Document(text=query, model="Qdrant/bm25"), using="sparse",
                                limit=initial_limit),
                models.Prefetch(query=query_vec, using="dense", limit=initial_limit),
            ],
            query=models.FusionQuery(fusion=models.Fusion.RRF),
            limit=initial_limit,
        )
        candidate_docs = [{"text": point.payload.get("text", ""), "source": point.payload.get("source", "unknown")} for
                          point in response.points]
    except Exception as e:
        print(f"Search Error: {e}")
        return []

    if not candidate_docs: return []
    top_results = rerank_documents(query, candidate_docs)[:final_limit]
    return top_results


def query_rewrite(query: str) -> str:
    prompt = f"ä½ æ˜¯ä¸€å€‹æœå°‹å¼•æ“å„ªåŒ–å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹ä½¿ç”¨è€…çš„å•é¡Œæ”¹å¯«ç‚ºæ›´ç²¾ç¢ºçš„é—œéµå­—æŸ¥è©¢ã€‚\nä½¿ç”¨è€…å•é¡Œ: {query}\næ”¹å¯«å¾ŒæŸ¥è©¢:"
    return llm_client.generate(prompt).strip()


def main():
    # 1. IDP è™•ç†
    docs_content = process_idp_files(FILE_PATH, FILE_LIST)

    # 2. åˆ‡åˆ† (Chunking) - ã€é †åºæ”¹è®Šï¼šå…ˆåˆ‡åˆ†ã€‘
    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", " ", ""], chunk_size=500, chunk_overlap=50
    )

    all_documents = []
    for filename, text in docs_content.items():
        all_documents.append(LCDocument(page_content=text, metadata={"source": filename}))

    split_docs = text_splitter.split_documents(all_documents)
    print(f"ğŸ“Š å…±åˆ‡åˆ†å‡º {len(split_docs)} å€‹å€å¡Š")

    # 3. å®‰å…¨æ€§æƒæ (Scanning) - ã€é †åºæ”¹è®Šï¼šå¾Œæƒæã€‘
    # é‡å°åˆ‡åˆ†å¾Œçš„ Chunk é€²è¡Œæƒæï¼Œé€™æ¨£æ‰èƒ½æŠ“åˆ° 5.docx çš„æƒ¡æ„ç‰‡æ®µ
    final_docs = scan_chunks_for_injection(split_docs)

    # 4. å¯«å…¥å‘é‡åº«
    init_qdrant_collection(final_docs)

    print(f"ğŸ“‚ è®€å–å•é¡Œ: {PREDICT_INPUT}")
    if not os.path.exists(PREDICT_INPUT):
        print("âŒ æª”æ¡ˆä¸å­˜åœ¨")
        return

    df = pd.read_csv(PREDICT_INPUT)

    # ã€æ¸¬è©¦æ¨¡å¼ã€‘åªè·‘å‰ 5 é¡Œ
    df = df.head(5)
    print(f"âš ï¸ æ¸¬è©¦æ¨¡å¼å•Ÿå‹•ï¼šåƒ…è™•ç†å‰ {len(df)} é¡Œè³‡æ–™")

    if 'answer' not in df.columns: df['answer'] = None
    if 'source' not in df.columns: df['source'] = None
    if 'id' not in df.columns:
        print("âŒ ç¼ºå°‘ id æ¬„ä½")
        return

    ground_truth_list = []

    print("ğŸš€ é–‹å§‹å›ç­”å•é¡Œ...")
    for index, row in tqdm(df.iterrows(), total=df.shape[0]):
        original_question = str(row['questions'])

        refined_query = query_rewrite(original_question)
        search_results = hybrid_search_with_rerank(refined_query)

        if search_results:
            retrieval_docs = [item[0] for item in search_results]
            context_str = "\n".join([doc['text'] for doc in retrieval_docs])
            unique_sources = sorted(list(set([doc['source'] for doc in retrieval_docs])))
            source_str = ",".join(unique_sources)

            if index < 1:
                print(f"\nğŸ” [Debug Context]: {context_str[:200]}...")
        else:
            context_str = ""
            source_str = ""
            retrieval_docs = []

        ground_truth_list.append({
            "id": row['id'],
            "questions": original_question,
            "contexts": [doc['text'] for doc in retrieval_docs],
            "ground_truth": ""
        })

        qa_prompt = f"""
        ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è³‡è¨ŠåŠ©æ‰‹ã€‚è«‹æ ¹æ“šã€åƒè€ƒè³‡æ–™ã€‘å›ç­”å•é¡Œã€‚
        è‹¥è³‡æ–™ä¸è¶³è«‹å›ç­”ã€Œç›®å‰è³‡è¨Šä¸è¶³ã€ã€‚
        è«‹ç›´æ¥å›ç­”é‡é»ï¼Œä¸è¦é‡è¤‡å•é¡Œã€‚

        ã€åƒè€ƒè³‡æ–™ã€‘ï¼š
        {context_str}

        ã€å•é¡Œã€‘ï¼š{original_question}
        ã€å›ç­”ã€‘ï¼š
        """
        answer = llm_client.generate(qa_prompt)

        df.at[index, 'answer'] = answer
        df.at[index, 'source'] = source_str

        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        gc.collect()

    df.to_csv(PREDICT_OUTPUT, index=False, encoding='utf-8-sig')
    print(f"âœ… çµæœå·²å„²å­˜: {PREDICT_OUTPUT}")

    pd.DataFrame(ground_truth_list).to_csv(GROUND_TRUTH_OUTPUT, index=False, encoding='utf-8-sig')
    print(f"âœ… GT å·²å„²å­˜: {GROUND_TRUTH_OUTPUT}")


if __name__ == "__main__":
    main()