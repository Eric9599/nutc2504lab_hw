import time
from typing import TypedDict, List, Annotated
from langchain_core.messages import BaseMessage, HumanMessage
from langgraph.graph import StateGraph, END, add_messages
from langchain_openai import ChatOpenAI
from search_searxng import search_searxng
from vlm_read_website import vlm_read_website

llm = ChatOpenAI(
    base_url="https://ws-02.wade0426.me/v1",
    api_key="day4hw",
    model="google/gemma-2-27b-it",
    temperature=0
)


class AgentState(TypedDict):
    input: str  # ä½¿ç”¨è€…å•é¡Œ
    knowledge_base: str  # å·²çŸ¥è³‡è¨Š
    messages: Annotated[list[BaseMessage], add_messages]  # éç¨‹ç´€éŒ„
    search_results: List[dict]  # æœå°‹çµæœæš«å­˜
    current_result_index: int  # ç›®å‰ VLM è®€åˆ°ç¬¬å¹¾ç¯‡
    vlm_temp_content: str  # VLM å‰›è®€å®Œçš„å…§å®¹
    final_answer: str  # æœ€çµ‚ç”¢å‡º
    is_sufficient: bool  # è³‡è¨Šæ˜¯å¦è¶³å¤ 
    search_exhausted: bool  # æœå°‹çµæœæ˜¯å¦çœ‹å®Œ
    valuable_found: bool  # VLM æ˜¯å¦ç™¼ç¾æœ‰åƒ¹å€¼è³‡è¨Š


def node_check_cache(state: AgentState):
    print("\nğŸ”¹ [Node] æª¢æŸ¥å¿«å–")
        # (æ­¤è™•å¯å¯¦ä½œ Redis/VectorDB)
    hit = False
    if hit:
        return {"final_answer": "Cached Answer"}
    return {}


def node_decision(state: AgentState):
    print("\n* [Node] æ±ºç­–å±¤è©•ä¼°")
    kb = state.get("knowledge_base", "ç„¡")

    system_prompt = f"""
    ä½¿ç”¨è€…å•é¡Œ: {state['input']}
    ç›®å‰æ”¶é›†åˆ°çš„è³‡è¨Š: {kb}

    è«‹è©•ä¼°ï¼šç›®å‰çš„è³‡è¨Šæ˜¯å¦è¶³ä»¥ã€Œå®Œæ•´ã€å›ç­”å•é¡Œï¼Ÿ
    - è‹¥è¶³å¤ ï¼Œè«‹æ ¹æ“šè³‡è¨Šç”Ÿæˆå›ç­”ã€‚
    - è‹¥ä¸è¶³ï¼Œè«‹å›è¦† 'SEARCH'ã€‚
    """
    response = llm.invoke(system_prompt).content

    if "SEARCH" in response:
        print("   => æ±ºå®šï¼šéœ€è¦æ›´å¤šè³‡è¨Š (SEARCH)")
        return {"is_sufficient": False}
    else:
        print("   => æ±ºå®šï¼šè³‡è¨Šè¶³å¤ ï¼Œç”Ÿæˆç­”æ¡ˆ")
        return {"is_sufficient": True, "final_answer": response}


def node_gen_keywords(state: AgentState):
    print("\n* [Node] ç”Ÿæˆé—œéµå­—")

    system_prompt = f"""
    æŒ‡ä»¤ï¼šä½ æ˜¯ä¸€å€‹æœå°‹å¼•æ“é—œéµå­—ç”¢ç”Ÿå™¨ã€‚
    ä»»å‹™ï¼šå°‡ä½¿ç”¨è€…çš„å•é¡Œè½‰æ›ç‚ºä¸€å€‹ç°¡å–®çš„ Google æœå°‹é—œéµå­—ã€‚

    ä½¿ç”¨è€…å•é¡Œï¼š{state['input']}

    åš´æ ¼é™åˆ¶ï¼š
    1. åªå‡†è¼¸å‡ºé—œéµå­— (ä¾‹å¦‚: "Apple stock news")
    2. ç¦æ­¢è¼¸å‡ºä»»ä½•æ¨™é»ç¬¦è™Ÿã€è§£é‡‹ã€å‰è¨€æˆ–å¾Œèªã€‚
    3. ç¦æ­¢è¼¸å‡º "Here is the keyword" ä¹‹é¡çš„å»¢è©±ã€‚
    4. ç›´æ¥çµ¦ç­”æ¡ˆï¼Œä¸è¦å›‰å—¦ã€‚
    """

    print("   (ç­‰å¾… LLM ç”Ÿæˆä¸­...)")
    try:
        response = llm.invoke(system_prompt, config={"timeout": 10})
        raw_content = response.content
    except Exception as e:
        print(f"* LLM ç”Ÿæˆè¶…æ™‚æˆ–éŒ¯èª¤ï¼Œä½¿ç”¨é è¨­é—œéµå­—ã€‚éŒ¯èª¤: {e}")
        raw_content = state['input']

    print(f"* [Debug] LLM åŸå§‹å›æ‡‰: {raw_content[:100]}...")

    keyword = raw_content.replace('"', '').replace("'", "").replace("\n", " ").strip()

    remove_words = ["search query:", "keyword:", "google search:", "here is"]
    for word in remove_words:
        if keyword.lower().startswith(word):
            keyword = keyword[len(word):].strip()

    if len(keyword) > 30:
        print(f"   [è­¦å‘Š] é—œéµå­—å¤ªé•·ï¼Œå¼·åˆ¶æˆªæ–·...")
        keyword = keyword[:30]

    print(f"   => æœ€çµ‚ä½¿ç”¨é—œéµå­—: {keyword}")
    return {"messages": [f"Query: {keyword}"]}


def node_search_tool(state: AgentState):
    print("\n* [Node] åŸ·è¡Œ searXNG æœå°‹...")

    messages = state.get('messages', [])
    if not messages:
        print("* éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è¨Šæ¯ç´€éŒ„ï¼Œç„¡æ³•æœå°‹ã€‚")
        return {"search_results": [], "current_result_index": 0}

    last_msg = messages[-1]

    if hasattr(last_msg, 'content'):
        query_text = last_msg.content
    else:
        query_text = str(last_msg)

    query = query_text.replace("Query: ", "").strip()

    print(f"   => æå–åˆ°çš„é—œéµå­—: {query}")

    try:
        results = search_searxng(query, limit=3)
        print(f"   => æ‰¾åˆ° {len(results)} ç­†è³‡æ–™")
    except Exception as e:
        print(f"* æœå°‹å·¥å…·å ±éŒ¯: {e}")
        results = []

    return {"search_results": results, "current_result_index": 0}


def node_vlm_process(state: AgentState):
    print("\n[Node] VLM è¦–è¦ºé–±è®€")
    idx = state.get("current_result_index", 0)
    results = state.get("search_results", [])

    if not results or idx >= len(results):
        print("   => ç„¡æ›´å¤šæœå°‹çµæœ")
        return {"search_exhausted": True}

    target = results[idx]

    # å‘¼å« vlm_read_website.py
    # å‚³å…¥ URL å’Œ Title å¹«åŠ© LLM ç†è§£
    content = vlm_read_website(target['url'], target.get('title', ''))

    return {"vlm_temp_content": content, "search_exhausted": False}


def node_value_check(state: AgentState):
    print("\n[Node] åƒ¹å€¼è©•ä¼°")
    content = state.get("vlm_temp_content", "")[:2000]

    system_prompt = f"""
    ä½¿ç”¨è€…å•é¡Œ: {state['input']}
    å‰›è®€å–çš„ç¶²é å…§å®¹æ‘˜è¦: {content}

    è«‹å•é€™æ®µå…§å®¹å°å›ç­”ä½¿ç”¨è€…å•é¡Œã€Œæœ‰å¹«åŠ©/æœ‰åƒ¹å€¼ã€å—ï¼Ÿ
    è«‹å›è¦† YES æˆ– NOã€‚
    """
    res = llm.invoke(system_prompt).content.upper()
    print(f"   => è©•ä¼°çµæœ: {res}")

    if "YES" in res:
        # å°‡æ–°è³‡è¨ŠåŠ å…¥çŸ¥è­˜åº«
        old_kb = state.get("knowledge_base", "")
        new_kb = f"{old_kb}\n\n[æ–°è³‡è¨Š]: {content}"
        return {"knowledge_base": new_kb, "valuable_found": True}
    else:
        return {"valuable_found": False}


def node_update_index(state: AgentState):
    return {"current_result_index": state["current_result_index"] + 1}


# draw graph
workflow = StateGraph(AgentState)
start_time = time.time()
workflow.add_node("check_cache", node_check_cache)
workflow.add_node("decision", node_decision)
workflow.add_node("gen_keywords", node_gen_keywords)
workflow.add_node("search_tool", node_search_tool)
workflow.add_node("vlm_process", node_vlm_process)
workflow.add_node("value_check", node_value_check)
workflow.add_node("update_index", node_update_index)
end_time = time.time()
workflow.set_entry_point("check_cache")

# é‚è¼¯é€£ç·š
workflow.add_conditional_edges(
    "check_cache",
    lambda x: "end" if x.get("final_answer") else "decision",
    {"end": END, "decision": "decision"}
)

workflow.add_conditional_edges(
    "decision",
    lambda x: "end" if x.get("is_sufficient") else "gen_keywords",
    {"end": END, "gen_keywords": "gen_keywords"}
)

workflow.add_edge("gen_keywords", "search_tool")
workflow.add_edge("search_tool", "vlm_process")

# VLM è®€å®Œå¾Œçš„è·¯ç”±ï¼šæ²’è³‡æ–™äº† -> å›æ±ºç­–å±¤ï¼›é‚„æœ‰è³‡æ–™ -> æª¢æŸ¥åƒ¹å€¼
workflow.add_conditional_edges(
    "vlm_process",
    lambda x: "decision" if x.get("search_exhausted") else "value_check",
    {"decision": "decision", "value_check": "value_check"}
)

# åƒ¹å€¼æª¢æŸ¥å¾Œçš„è·¯ç”±ï¼šæœ‰åƒ¹å€¼ -> å›æ±ºç­–å±¤é‡åˆ¤ï¼›æ²’åƒ¹å€¼ -> è®€ä¸‹ä¸€ç¯‡
workflow.add_conditional_edges(
    "value_check",
    lambda x: "decision" if x.get("valuable_found") else "update_index",
    {"decision": "decision", "update_index": "update_index"}
)

workflow.add_edge("update_index", "vlm_process")

app = workflow.compile()
print(app.get_graph().draw_ascii())


# Executing
if __name__ == "__main__":
    print("* AI Agent å•Ÿå‹•ä¸­...")
    q = input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ: ")

    initial = {
        "input": q,
        "knowledge_base": "",
        "messages": [],
        "search_results": [],
        "current_result_index": 0
    }

    final_output = None

    print("\n--- é–‹å§‹è™•ç† ---")

    for event in app.stream(initial):
        for value in event.values():
            # å¦‚æœç¯€é»æ²’æœ‰å›å‚³ä»»ä½•æ±è¥¿ (None)ï¼Œå°±è·³éã€‚
            if value and "final_answer" in value:
                final_output = value["final_answer"]

    print("\n" + "=" * 30)
    print("* æœ€çµ‚ç­”æ¡ˆï¼š")
    print("=" * 30)

    if final_output:
        print(final_output)
    else:
        print("* ä»»å‹™çµæŸï¼Œä½†æ²’æœ‰ç”Ÿæˆç­”æ¡ˆã€‚")

    print("\n" + "=" * 30)