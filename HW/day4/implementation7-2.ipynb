{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-05T06:23:45.375831Z",
     "start_time": "2026-02-05T06:23:45.347867Z"
    }
   },
   "source": [
    "import json\n",
    "from typing import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import  HumanMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "import os\n",
    "import time\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T06:23:45.438344Z",
     "start_time": "2026-02-05T06:23:45.376663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://ws-02.wade0426.me/v1\",\n",
    "    api_key=\"day3hw\",\n",
    "    model=\"google/gemma-3-27b-it\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "fast_llm = ChatOpenAI(\n",
    "    base_url=\"https://ws-05.huannage.com/v1\",\n",
    "    api_key=\"day3hw\",\n",
    "    model=\"Qwen3-VL-8B-Instruct-BF16.gguf\",\n",
    "    temperature=0\n",
    ")"
   ],
   "id": "94f5072d52f57ac0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T06:23:45.511075Z",
     "start_time": "2026-02-05T06:23:45.439342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# è¨­å®šå¿«å–æª”æ¡ˆåç¨±\n",
    "CACHE_FILE = \"qa_cache.json\"\n",
    "\n",
    "def get_clean_key(text: str) -> str:\n",
    "    \"\"\"çµ±ä¸€å°‡å•é¡Œæ¨™æº–åŒ–\"\"\"\n",
    "    return text.replace(\" \", \"\").replace(\"?\", \"\")\n",
    "\n",
    "def load_cache():\n",
    "    \"\"\"å¾ JSON è®€å–å¿«å–è³‡æ–™\"\"\"\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    if not os.path.exists(CACHE_FILE):\n",
    "        default_data = {\n",
    "            get_clean_key(\"LangGraphæ˜¯ä»€éº¼\"): \"LangGraph æ˜¯ä¸€å€‹ç”¨æ–¼æ§‹å»ºæœ‰ç‹€æ…‹ã€å¤šåƒèˆ‡è€…æ‡‰ç”¨ç¨‹å¼...\",\n",
    "            get_clean_key(\"ä½ çš„åå­—\"): \"æˆ‘æ˜¯é€™å€‹èª²ç¨‹çš„ AI åŠ©æ•™ã€‚\"\n",
    "        }\n",
    "        with open(CACHE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(default_data, f, ensure_ascii=False, indent=4)\n",
    "        return default_data\n",
    "\n",
    "    try:\n",
    "        with open(CACHE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}"
   ],
   "id": "4cb7d58c7f858b34",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T06:23:45.528425Z",
     "start_time": "2026-02-05T06:23:45.514304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_cache(new_data: dict):\n",
    "    \"\"\"å°‡è³‡æ–™å¯«å…¥ JSON\"\"\"\n",
    "    current_data = {}\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        try:\n",
    "            with open(CACHE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                current_data = json.load(f)\n",
    "        except:\n",
    "            pass\n",
    "    current_data.update(new_data)\n",
    "    with open(CACHE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(current_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# 1. å®šç¾©ç‹€æ…‹\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    source: str # CACHE / FAST / LLM\n",
    "\n",
    "def check_cache_node(state: State):\n",
    "    \"\"\"æª¢æŸ¥å¿«å–\"\"\"\n",
    "    print(f\"\\n[ç³»çµ±] æ”¶åˆ°å•é¡Œ: {state['question']}\")\n",
    "    cache_data = load_cache()\n",
    "    clean_query = get_clean_key(state['question'])\n",
    "\n",
    "    if clean_query in cache_data:\n",
    "        print(\"--- å‘½ä¸­å¿«å– (Cache Hit) ---\")\n",
    "        return {\n",
    "            \"answer\": cache_data[clean_query],\n",
    "            \"source\": \"CACHE\"\n",
    "        }\n",
    "    else:\n",
    "        print(\"--- å¿«å–æœªå‘½ä¸­ (Cache Miss) ---\")\n",
    "        return {\"source\": \"MISS\"}\n",
    "\n",
    "def fast_reply_node(state: State):\n",
    "    print(\"--- é€²å…¥å¿«é€Ÿé€šé“ (Fast Track API) ---\")\n",
    "\n",
    "    response = fast_llm.invoke([HumanMessage(content=state['question'])])\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.content,\n",
    "        \"source\": \"FAST_TRACK_API\"\n",
    "    }\n",
    "\n",
    "def expert_node(state: State):\n",
    "    \"\"\"\n",
    "    æ…¢é€Ÿé€šé“ï¼šå‘¼å« LLM ä¸¦ä½¿ç”¨ã€Œæµå¼å‚³è¼¸ã€\n",
    "    \"\"\"\n",
    "    print(\"--- é€²å…¥å°ˆå®¶æ¨¡å¼ (LLM Expert) ---\")\n",
    "\n",
    "    prompt = f\"è«‹ä»¥å°ˆæ¥­çš„è§’åº¦å›ç­”ä»¥ä¸‹å•é¡Œ: {state['question']}\"\n",
    "\n",
    "    chunks = llm.stream([HumanMessage(content=prompt)])\n",
    "\n",
    "    full_answer = \"\"\n",
    "    print(\"ğŸ¤– AI æ­£åœ¨æ€è€ƒä¸¦æ‰“å­—: \", end=\"\", flush=True)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        content = chunk.content\n",
    "        if content:\n",
    "            print(content, end=\"\", flush=True)\n",
    "            full_answer += content\n",
    "    print(\"\\n\")\n",
    "\n",
    "    clean_key = get_clean_key(state['question'])\n",
    "    save_cache({clean_key: full_answer})\n",
    "    print(f\"--- [ç³»çµ±] å·²å°‡å®Œæ•´å›ç­”å¯«å…¥ {CACHE_FILE} ---\")\n",
    "\n",
    "    return {\n",
    "        \"answer\": full_answer,\n",
    "        \"source\": \"LLM_EXPERT\"\n",
    "    }"
   ],
   "id": "d4bfce4dc883ccff",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T06:28:32.625482Z",
     "start_time": "2026-02-05T06:24:31.841734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def expert_node(state: State):\n",
    "    \"\"\"\n",
    "    æ…¢é€Ÿé€šé“ï¼šå‘¼å« LLM ä¸¦ä½¿ç”¨ã€Œæµå¼å‚³è¼¸ã€\n",
    "    \"\"\"\n",
    "    print(\"--- é€²å…¥å°ˆå®¶æ¨¡å¼ (LLM Expert) ---\")\n",
    "\n",
    "    prompt = f\"è«‹ä»¥å°ˆæ¥­çš„è§’åº¦å›ç­”ä»¥ä¸‹å•é¡Œ: {state['question']}\"\n",
    "\n",
    "    chunks = llm.stream([HumanMessage(content=prompt)])\n",
    "\n",
    "    full_answer = \"\"\n",
    "    print(\"ğŸ¤– AI æ­£åœ¨æ€è€ƒä¸¦æ‰“å­—: \", end=\"\", flush=True)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        content = chunk.content\n",
    "        if content:\n",
    "            print(content, end=\"\", flush=True)\n",
    "            full_answer += content\n",
    "    print(\"\\n\")\n",
    "\n",
    "    clean_key = get_clean_key(state['question'])\n",
    "    save_cache({clean_key: full_answer})\n",
    "    print(f\"--- [ç³»çµ±] å·²å°‡å®Œæ•´å›ç­”å¯«å…¥ {CACHE_FILE} ---\")\n",
    "\n",
    "    return {\n",
    "        \"answer\": full_answer,\n",
    "        \"source\": \"LLM_EXPERT\"\n",
    "    }\n",
    "\n",
    "def master_router(state: State):\n",
    "    \"\"\"ä¸»è·¯ç”±æ§åˆ¶å™¨\"\"\"\n",
    "    if state.get(\"answer\"):\n",
    "        return \"end\"\n",
    "\n",
    "    question = state['question']\n",
    "    # åªè¦æœ‰é€™äº›æ‰“æ‹›å‘¼çš„è©ï¼Œå°±èµ° Fast Track (ws-05)\n",
    "    if any(word in question for word in [\"ä½ å¥½\", \"å—¨\", \"æ—©å®‰\", \"å“ˆå›‰\"]):\n",
    "        return \"fast\"\n",
    "    else:\n",
    "        return \"expert\"\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"check_cache\", check_cache_node)\n",
    "workflow.add_node(\"fast_bot\", fast_reply_node)\n",
    "workflow.add_node(\"expert_bot\", expert_node)\n",
    "\n",
    "workflow.set_entry_point(\"check_cache\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_cache\",\n",
    "    master_router,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"fast\": \"fast_bot\",\n",
    "        \"expert\": \"expert_bot\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"fast_bot\", END)\n",
    "workflow.add_edge(\"expert_bot\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "print(app.get_graph().draw_ascii())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"å¿«å–æª”æ¡ˆå°‡å„²å­˜æ–¼: {os.path.abspath(CACHE_FILE)}\")\n",
    "    print(\"æç¤ºï¼šè©¦è‘—è¼¸å…¥ 'ä½ å¥½' æ¸¬è©¦ Fast APIï¼Œè¼¸å…¥å°ˆæ¥­å•é¡Œæ¸¬è©¦ Expert APIã€‚\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nè«‹è¼¸å…¥å•é¡Œ (è¼¸å…¥ q é›¢é–‹): \")\n",
    "        if user_input.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        inputs = {\"question\": user_input}\n",
    "\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = app.invoke(inputs)\n",
    "            end_time = time.time()\n",
    "\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"ä¾†æº: [{result['source']}]\")\n",
    "            print(f\"è€—æ™‚: {end_time - start_time:.4f} ç§’\")\n",
    "\n",
    "            # åªæœ‰é Expert (Cache æˆ– Fast API) éœ€è¦åœ¨é€™è£¡å°å‡ºçµæœ\n",
    "            if result['source'] != \"LLM_EXPERT\":\n",
    "                print(f\"å›ç­”: {result['answer']}\")\n",
    "            else:\n",
    "                print(\"(å›ç­”å·²æ–¼ä¸Šæ–¹æµå¼è¼¸å‡ºå®Œç•¢)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ç™¼ç”ŸéŒ¯èª¤: {e}\")"
   ],
   "id": "77a5a9c417920505",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        +-----------+                   \n",
      "                        | __start__ |                   \n",
      "                        +-----------+                   \n",
      "                               *                        \n",
      "                               *                        \n",
      "                               *                        \n",
      "                       +-------------+                  \n",
      "                       | check_cache |                  \n",
      "                      .+-------------+...               \n",
      "                 .....         .         ....           \n",
      "              ...              .             ....       \n",
      "           ...                 .                 ....   \n",
      "+------------+           +----------+                .. \n",
      "| expert_bot |           | fast_bot |            ....   \n",
      "+------------+***        +----------+        ....       \n",
      "                 *****         *         ....           \n",
      "                      ***      *     ....               \n",
      "                         ***   *   ..                   \n",
      "                         +---------+                    \n",
      "                         | __end__ |                    \n",
      "                         +---------+                    \n",
      "å¿«å–æª”æ¡ˆå°‡å„²å­˜æ–¼: /Users/ericwu/Desktop/git_praticing/HW/day4/qa_cache.json\n",
      "æç¤ºï¼šè©¦è‘—è¼¸å…¥ 'ä½ å¥½' æ¸¬è©¦ Fast APIï¼Œè¼¸å…¥å°ˆæ¥­å•é¡Œæ¸¬è©¦ Expert APIã€‚\n",
      "\n",
      "[ç³»çµ±] æ”¶åˆ°å•é¡Œ: ä½ å¥½\n",
      "--- å¿«å–æœªå‘½ä¸­ (Cache Miss) ---\n",
      "--- é€²å…¥å¿«é€Ÿé€šé“ (Fast Track API) ---\n",
      "ç™¼ç”ŸéŒ¯èª¤: Connection error.\n",
      "\n",
      "[ç³»çµ±] æ”¶åˆ°å•é¡Œ: ä»€éº¼æ˜¯ è€³æ©Ÿï¼Ÿ\n",
      "--- å¿«å–æœªå‘½ä¸­ (Cache Miss) ---\n",
      "--- é€²å…¥å°ˆå®¶æ¨¡å¼ (LLM Expert) ---\n",
      "ğŸ¤– AI æ­£åœ¨æ€è€ƒä¸¦æ‰“å­—: è€³æ©Ÿï¼Œä¹Ÿç¨±ç‚ºé ­æˆ´å¼è€³æ©Ÿæˆ–è€³éº¥ï¼Œæ˜¯ä¸€ç¨®ç”¨æ–¼æ”¶è½éŸ³è¨Šçš„è¨­å‚™ã€‚å®ƒé€šå¸¸ç”±ä»¥ä¸‹å¹¾å€‹ä¸»è¦éƒ¨åˆ†çµ„æˆï¼š\n",
      "\n",
      "* **æšè²å™¨é©…å‹•å–®å…ƒï¼ˆSpeaker Driversï¼‰ï¼š** é€™æ˜¯è€³æ©Ÿçš„æ ¸å¿ƒï¼Œè² è²¬å°‡é›»ä¿¡è™Ÿè½‰æ›ç‚ºå¯è½çš„è²éŸ³ã€‚ä¸åŒé¡å‹çš„é©…å‹•å–®å…ƒï¼ˆä¾‹å¦‚å‹•åœˆã€å¹³é¢éœ‡è†œç­‰ï¼‰æœƒå½±éŸ¿éŸ³è³ªã€‚\n",
      "* **å¤–æ®¼ï¼ˆHousingï¼‰ï¼š** ç”¨æ–¼ä¿è­·å…§éƒ¨å…ƒä»¶ï¼Œä¸¦å½±éŸ¿è²éŸ³çš„å‚³éå’Œéš”éŸ³æ•ˆæœã€‚\n",
      "* **è€³ç½©/è€³å¡ï¼ˆEarcups/Eartipsï¼‰ï¼š** èˆ‡è€³æœµç›´æ¥æ¥è§¸çš„éƒ¨åˆ†ï¼Œæä¾›èˆ’é©æ€§å’Œéš”éŸ³æ•ˆæœã€‚è€³ç½©å¯ä»¥æ˜¯åŒ…è£¹å¼ï¼ˆOver-earï¼‰æˆ–è€³å…§å¼ï¼ˆOn-earï¼‰ï¼Œè€³å¡å‰‡ç›´æ¥æ’å…¥è€³é“ã€‚\n",
      "* **é ­å¸¶ï¼ˆHeadbandï¼‰ï¼š** ç”¨æ–¼å›ºå®šè€³æ©Ÿåœ¨é ­éƒ¨ã€‚\n",
      "* **é€£æ¥ç·š/ç„¡ç·šé€£æ¥ï¼ˆCable/Wireless Connectionï¼‰ï¼š** å°‡è€³æ©Ÿèˆ‡éŸ³è¨Šä¾†æºï¼ˆä¾‹å¦‚æ‰‹æ©Ÿã€é›»è…¦ï¼‰é€£æ¥ã€‚æœ‰ç·šè€³æ©Ÿä½¿ç”¨é€£æ¥ç·šï¼Œç„¡ç·šè€³æ©Ÿå‰‡ä½¿ç”¨è—ç‰™æˆ–å…¶ä»–ç„¡ç·šæŠ€è¡“ã€‚\n",
      "* **éº¥å…‹é¢¨ï¼ˆMicrophoneï¼Œéƒ¨åˆ†è€³æ©Ÿï¼‰ï¼š** ç”¨æ–¼èªéŸ³é€šè©±æˆ–éŒ„éŸ³ã€‚\n",
      "\n",
      "**ç°¡å–®ä¾†èªªï¼Œè€³æ©Ÿå°±æ˜¯ä¸€å€‹å°‡é›»ä¿¡è™Ÿè½‰æ›ç‚ºè²éŸ³ä¸¦å‚³é€åˆ°ä½ è€³æœµçš„è¨­å‚™ï¼Œè®“ä½ èƒ½å¤ è½åˆ°éŸ³æ¨‚ã€å»£æ’­ã€éŠæˆ²éŸ³æ•ˆæˆ–å…¶ä»–éŸ³è¨Šå…§å®¹ã€‚** å®ƒå€‘æœ‰å„ç¨®å„æ¨£çš„é¡å‹ï¼ŒåŒ…æ‹¬ï¼š\n",
      "\n",
      "* **æœ‰ç·šè€³æ©Ÿï¼š** ä½¿ç”¨é€£æ¥ç·šï¼ŒéŸ³è³ªç©©å®šï¼Œé€šå¸¸åƒ¹æ ¼è¼ƒä½ã€‚\n",
      "* **ç„¡ç·šè€³æ©Ÿï¼š** ä½¿ç”¨è—ç‰™æˆ–å…¶ä»–ç„¡ç·šæŠ€è¡“ï¼Œæ–¹ä¾¿ç§»å‹•ï¼Œä½†éœ€è¦å……é›»ã€‚\n",
      "* **é–‹æ”¾å¼è€³æ©Ÿï¼š** è²éŸ³æ´©éœ²è¼ƒå¤šï¼Œæä¾›æ›´è‡ªç„¶çš„éŸ³å ´ã€‚\n",
      "* **é–‰åˆå¼è€³æ©Ÿï¼š** éš”éŸ³æ•ˆæœå¥½ï¼Œæä¾›æ›´æ²‰æµ¸çš„è†è½é«”é©—ã€‚\n",
      "* **é™å™ªè€³æ©Ÿï¼š** å…·æœ‰ä¸»å‹•é™å™ªåŠŸèƒ½ï¼Œå¯ä»¥æœ‰æ•ˆæ¶ˆé™¤ç’°å¢ƒå™ªéŸ³ã€‚\n",
      "\n",
      "å¸Œæœ›é€™å€‹è§£é‡‹å°æ‚¨æœ‰æ‰€å¹«åŠ©ï¼\n",
      "<|im_end|>\n",
      "\n",
      "--- [ç³»çµ±] å·²å°‡å®Œæ•´å›ç­”å¯«å…¥ qa_cache.json ---\n",
      "------------------------------\n",
      "ä¾†æº: [LLM_EXPERT]\n",
      "è€—æ™‚: 189.9234 ç§’\n",
      "(å›ç­”å·²æ–¼ä¸Šæ–¹æµå¼è¼¸å‡ºå®Œç•¢)\n",
      "\n",
      "[ç³»çµ±] æ”¶åˆ°å•é¡Œ: q\n",
      "\n",
      "--- å¿«å–æœªå‘½ä¸­ (Cache Miss) ---\n",
      "--- é€²å…¥å°ˆå®¶æ¨¡å¼ (LLM Expert) ---\n",
      "ğŸ¤– AI æ­£åœ¨æ€è€ƒä¸¦æ‰“å­—: Okay, I understand. I will do my best to answer your questions in a thorough, professional manner, drawing on a wide range of knowledge. Please present your question. I am ready when you are.\n",
      "<|im_end|>\n",
      "\n",
      "--- [ç³»çµ±] å·²å°‡å®Œæ•´å›ç­”å¯«å…¥ qa_cache.json ---\n",
      "------------------------------\n",
      "ä¾†æº: [LLM_EXPERT]\n",
      "è€—æ™‚: 16.7908 ç§’\n",
      "(å›ç­”å·²æ–¼ä¸Šæ–¹æµå¼è¼¸å‡ºå®Œç•¢)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 71\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mæç¤ºï¼šè©¦è‘—è¼¸å…¥ \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mä½ å¥½\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m æ¸¬è©¦ Fast APIï¼Œè¼¸å…¥å°ˆæ¥­å•é¡Œæ¸¬è©¦ Expert APIã€‚\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m---> 71\u001B[0m     user_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mè«‹è¼¸å…¥å•é¡Œ (è¼¸å…¥ q é›¢é–‹): \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m user_input\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mq\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     73\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/ipykernel/kernelbase.py:1396\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[0;34m(self, prompt)\u001B[0m\n\u001B[1;32m   1394\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1395\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[0;32m-> 1396\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1397\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1398\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_shell_context_var\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_shell_parent_ident\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1399\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1400\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1401\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/ipykernel/kernelbase.py:1441\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[0;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[1;32m   1438\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m   1439\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[1;32m   1440\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1442\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m   1443\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
