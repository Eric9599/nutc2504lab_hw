{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-11T05:16:20.093632Z",
     "start_time": "2026-02-11T05:16:20.023531Z"
    }
   },
   "source": [
    "import pdfplumber\n",
    "from llm_guard.input_scanners import PromptInjection, InvisibleText\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from llm_guard.input_scanners.prompt_injection import MatchType\n",
    "import pytesseract\n",
    "from docx import Document\n",
    "from PIL import Image\n",
    "from langchain_core.documents import Document as LCDocument\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from deepeval.metrics import (\n",
    "    FaithfulnessMetric,\n",
    "    AnswerRelevancyMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRelevancyMetric\n",
    ")\n",
    "\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient, models\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from typing import List\n",
    "import requests\n",
    "import uuid\n",
    "import gc\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T05:16:20.269656Z",
     "start_time": "2026-02-11T05:16:20.143294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_list = [\"1.pdf\", \"2.pdf\", \"3.pdf\", \"4.png\", \"5.docx\"]\n",
    "file_path = \"HW\"\n",
    "EMBEDDING_API_URL = \"http://ws-04.wade0426.me/embed\"\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "COLLECTION_NAME = \"rag_homework_day7_api\"\n",
    "LLM_BASE_URL = \"https://ws-06.huannago.com/v1\"\n",
    "RERANKER_MODEL_PATH = os.path.expanduser(\"../day6/Qwen3-Reranker-0.6B\")\n",
    "LLM_API_KEY = \"day6hw\"\n",
    "LLM_MODEL_NAME = \"gemma-3-27b-it\"\n",
    "PREDICT_INPUT = \"HW/questions.csv\"\n",
    "PREDICT_OUPUT = \"HW/output.csv\""
   ],
   "id": "5e5d809f28b772e8",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T05:16:48.897266Z",
     "start_time": "2026-02-11T05:16:20.272239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_list = [\"1.pdf\", \"2.pdf\", \"3.pdf\", \"4.png\", \"5.docx\"]\n",
    "file_path = \"HW\"\n",
    "docs_content = {}\n",
    "\n",
    "print(\"--- ÈñãÂßã IDP ËôïÁêÜ ---\")\n",
    "\n",
    "for file_name in file_list:\n",
    "    full_path = os.path.join(file_path, file_name)\n",
    "\n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"* Êâæ‰∏çÂà∞Ê™îÊ°à: {file_name}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Ê≠£Âú®ËôïÁêÜ: {file_name}...\")\n",
    "    extracted_text = \"\"\n",
    "\n",
    "    # ==========================================\n",
    "    # 1. ÈáùÂ∞ç PDF (ÂåÖÂê´ 3.pdf ÁöÑË°®Ê†ºËôïÁêÜ)\n",
    "    # ==========================================\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        try:\n",
    "            with pdfplumber.open(full_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        extracted_text += text + \"\\n\"\n",
    "\n",
    "                    tables = page.extract_tables()\n",
    "                    for table in tables:\n",
    "                        table_str = \"\"\n",
    "                        for row in table:\n",
    "                            # Ê∏ÖÊ¥ó row ‰∏≠ÁöÑ NoneÔºåËΩâÁÇ∫Â≠ó‰∏≤\n",
    "                            cleaned_row = [str(cell) if cell is not None else \"\" for cell in row]\n",
    "                            table_str += \" | \".join(cleaned_row) + \"\\n\"\n",
    "\n",
    "                        extracted_text += \"\\n[Ë°®Ê†ºÂÖßÂÆπÈñãÂßã]\\n\" + table_str + \"\\n[Ë°®Ê†ºÂÖßÂÆπÁµêÊùü]\\n\"\n",
    "\n",
    "                print(f\"‚úÖ [{file_name}] PDF (Âê´Ë°®Ê†º) ÊèêÂèñÊàêÂäü\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå PDF ËÆÄÂèñÈåØË™§ {file_name}: {e}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. ÈáùÂ∞ç PNG (ÈáùÂ∞ç 4.png ÁöÑ OCR ËôïÁêÜ)\n",
    "    # ==========================================\n",
    "    elif file_name.endswith(\".png\"):\n",
    "        try:\n",
    "            image = Image.open(full_path)\n",
    "            ocr_text = pytesseract.image_to_string(image, lang='chi_tra+eng')\n",
    "\n",
    "            extracted_text = ocr_text\n",
    "            print(f\"‚úÖ [{file_name}] OCR Ëæ®Ë≠òÊàêÂäü\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå OCR Â§±Êïó {file_name}: {e}\")\n",
    "            print(\"ÊèêÁ§∫: Ë´ãÁ¢∫Ë™çÊòØÂê¶Â∑≤ÂÆâË£ù Tesseract ËªüÈ´î‰∏¶Ë®≠ÂÆöÁí∞Â¢ÉËÆäÊï∏\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. ÈáùÂ∞ç DOCX (5.docx)\n",
    "    # ==========================================\n",
    "    elif file_name.endswith(\".docx\"):\n",
    "        try:\n",
    "            doc = Document(full_path)\n",
    "            for para in doc.paragraphs:\n",
    "                extracted_text += para.text + \"\\n\"\n",
    "            print(f\"‚úÖ [{file_name}] Word ËÆÄÂèñÊàêÂäü\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Word ËÆÄÂèñÈåØË™§ {file_name}: {e}\")\n",
    "\n",
    "    # --- Â≠òÂÖ•ÁµêÊûú ---\n",
    "    if extracted_text.strip():\n",
    "        docs_content[file_name] = extracted_text\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Ë≠¶Âëä: {file_name} Ê≤íÊúâÊèêÂèñÂà∞‰ªª‰ΩïÊñáÂ≠ó\")\n",
    "\n",
    "print(\"-\" * 30)"
   ],
   "id": "8be1e2c46e3fceb0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ [4.png] OCR Ëæ®Ë≠òÊàêÂäü\n",
      "Ê≠£Âú®ËôïÁêÜ: 5.docx...\n",
      "‚úÖ [5.docx] Word ËÆÄÂèñÊàêÂäü\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T05:16:48.984069Z",
     "start_time": "2026-02-11T05:16:48.935328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. ÂàùÂßãÂåñÂàáÂàÜÂô®\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # ÂàÜÈöîÁ¨¶ÂÑ™ÂÖàÈ†ÜÂ∫èÔºöÈõôÊèõË°å(ÊÆµËêΩ) > ÂñÆÊèõË°å(Ë°å) > Á©∫Ê†º > Á©∫Â≠ó‰∏≤\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=500,    # ÊØèÂÄãÂçÄÂ°äÂ§ßÁ¥Ñ 500 ÂÄãÂ≠óÂÖÉ\n",
    "    chunk_overlap=50,  # ÊªëÂãïË¶ñÁ™óÔºö‰øùÁïô 50 ÂÄãÂ≠óÂÖÉÁöÑÈáçÁñäÔºåÈÅøÂÖçË™ûÊÑèÊñ∑Ë£Ç\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# 2. Ê∫ñÂÇôÂ∞á‰Ω†ÁöÑÂ≠óÂÖ∏ËΩâÊèõÊàê LangChain ÁöÑ Document Áâ©‰ª∂\n",
    "all_documents = []\n",
    "\n",
    "for filename, text in docs_content.items():\n",
    "    # Âª∫Á´ã Document Áâ©‰ª∂ÔºåË®òÂæóÊääÊ™îÂêçÊîæÂÖ• metadataÔºåÈÄôÊòØ‰πãÂæå RAG ÂºïÁî®‰æÜÊ∫ê(Source)ÁöÑÈóúÈçµ\n",
    "    doc = LCDocument(\n",
    "        page_content=text,\n",
    "        metadata={\"source\": filename}\n",
    "    )\n",
    "    all_documents.append(doc)\n",
    "\n",
    "# 3. Âü∑Ë°åÂàáÂàÜ\n",
    "split_docs = text_splitter.split_documents(all_documents)"
   ],
   "id": "e067dbb876bbc86d",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T05:16:50.981395Z",
     "start_time": "2026-02-11T05:16:48.999621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scanner = PromptInjection(threshold=0.5, match_type=MatchType.FULL)\n",
    "for filename, text_content in docs_content.items():\n",
    "    print(f\"Ê≠£Âú®ÊéÉÊèè {filename} ...\")\n",
    "\n",
    "    # Á¢∫‰øùÂÖßÂÆπÊòØÂ≠ó‰∏≤ÔºåÈÅøÂÖçÈåØË™§\n",
    "    if not isinstance(text_content, str):\n",
    "        text_content = str(text_content)\n",
    "\n",
    "    # Âü∑Ë°åÊéÉÊèè\n",
    "    sanitized_text, is_valid, score = scanner.scan(text_content)\n",
    "\n",
    "    if not is_valid:\n",
    "        print(f\"*  Ë≠¶ÂëäÔºöÂú® [{filename}] ÁôºÁèæÈö±ËóèÊñáÂ≠óÔºÅ(Score: {score})\")\n",
    "    else:\n",
    "        print(f\"OKÔºö[{filename}] Êú™ÁôºÁèæÈö±ËóèÊñáÂ≠ó„ÄÇ\")"
   ],
   "id": "2bfafa1a871e2d78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKÔºö[5.docx] Êú™ÁôºÁèæÈö±ËóèÊñáÂ≠ó„ÄÇ\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T05:16:51.091543Z",
     "start_time": "2026-02-11T05:16:51.039815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_obj = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_obj = torch.device(\"mps\")\n",
    "else:\n",
    "    device_obj = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"‚è≥ ‰ΩøÁî®Ë£ùÁΩÆ: {device_obj}\")"
   ],
   "id": "8561629d9add3c24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ‰ΩøÁî®Ë£ùÁΩÆ: mps\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T05:17:00.593296Z",
     "start_time": "2026-02-11T05:16:51.108539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomAPIEmbeddings(Embeddings):\n",
    "    def __init__(self, api_url):\n",
    "        self.api_url = api_url\n",
    "\n",
    "    def _call_api(self, texts: List[str]) -> List[List[float]]:\n",
    "        data = {\n",
    "            \"texts\": texts,\n",
    "            \"normalize\": True,\n",
    "            \"batch_size\": 32\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(self.api_url, json=data, timeout=60)\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                return result.get('embeddings', [])\n",
    "            else:\n",
    "                print(f\"‚ùå API Error Code: {response.status_code}\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå API Exception: {e}\")\n",
    "            return []\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self._call_api(texts)\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        results = self._call_api([text])\n",
    "        if results and len(results) > 0:\n",
    "            return results[0]\n",
    "        return []\n",
    "\n",
    "\n",
    "print(f\"* ÂàùÂßãÂåñ Embedding API ({EMBEDDING_API_URL})...\")\n",
    "embedding_model = CustomAPIEmbeddings(EMBEDDING_API_URL)\n",
    "try:\n",
    "    print(\"* Ê∏¨Ë©¶ Embedding API ÈÄ£Á∑ö...\")\n",
    "    test_vec = embedding_model.embed_query(\"Ê∏¨Ë©¶\")\n",
    "    if test_vec:\n",
    "        print(f\"‚úÖ API ÈÄ£Á∑öÊàêÂäüÔºÅÂêëÈáèÁ∂≠Â∫¶: {len(test_vec)}\")\n",
    "    else:\n",
    "        print(\"‚ùå API ÈÄ£Á∑öÂ§±ÊïóÔºåÂõûÂÇ≥ÁÇ∫Á©∫\")\n",
    "        exit()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå API Ê∏¨Ë©¶ÁôºÁîü‰æãÂ§ñÈåØË™§: {e}\")\n",
    "    exit()"
   ],
   "id": "9558d81a60af039c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API ÈÄ£Á∑öÊàêÂäüÔºÅÂêëÈáèÁ∂≠Â∫¶: 4096\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T05:17:00.674055Z",
     "start_time": "2026-02-11T05:17:00.625115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleLLMClient:\n",
    "    def __init__(self, base_url, model_name, api_key):\n",
    "        self.client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.3,\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"LLM Error: {e}\")\n",
    "            return \"Error generating response.\"\n",
    "\n",
    "\n",
    "print(\"‚è≥ ÂàùÂßãÂåñ LLM Client...\")\n",
    "llm_client = SimpleLLMClient(LLM_BASE_URL, LLM_MODEL_NAME, LLM_API_KEY)\n"
   ],
   "id": "bc89be58c5f629d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ÂàùÂßãÂåñ LLM Client...\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T05:17:00.743394Z",
     "start_time": "2026-02-11T05:17:00.714805Z"
    }
   },
   "cell_type": "code",
   "source": "client = QdrantClient(url=QDRANT_URL)",
   "id": "d14de3997c7c333b",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T05:17:00.756293Z",
     "start_time": "2026-02-11T05:17:00.744515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def init_qdrant_collection(documents):\n",
    "    \"\"\"\n",
    "    Âª∫Á´ã Qdrant Collection ‰∏¶ÂØ´ÂÖ•Â∑≤ÂàáÂàÜÁöÑË≥áÊñô\n",
    "    Args:\n",
    "        documents: List[Document] (Áî± LangChain ÂàáÂàÜÂ•ΩÁöÑÊñá‰ª∂ÂàóË°®)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Ê™¢Êü•Ë≥áÊñôÊòØÂê¶ÁÇ∫Á©∫\n",
    "    if not documents:\n",
    "        print(\"‚ö†Ô∏è ÂÇ≥ÂÖ•ÁöÑÊñá‰ª∂ÂàóË°®ÁÇ∫Á©∫ÔºåË∑≥ÈÅéÂØ´ÂÖ•„ÄÇ\")\n",
    "        return\n",
    "\n",
    "    print(f\"üîÑ Ê≠£Âú®ÈáçÁΩÆÈõÜÂêà {COLLECTION_NAME}...\")\n",
    "    try:\n",
    "        client.delete_collection(COLLECTION_NAME)\n",
    "        print(f\"* Â∑≤Âà™Èô§ËàäÈõÜÂêà {COLLECTION_NAME}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2. Âª∫Á´ãÊñ∞ÁöÑ Collection\n",
    "    # Ê≥®ÊÑè: size=4096 ÂøÖÈ†àËàá‰Ω†‰ΩøÁî®ÁöÑ Embedding Ê®°ÂûãÁ∂≠Â∫¶‰∏ÄËá¥\n",
    "    # (‰æãÂ¶Ç OpenAI text-embedding-3-large ÊòØ 3072, text-embedding-ada-002 ÊòØ 1536)\n",
    "    print(f\"‚è≥ Âª∫Á´ãÊñ∞ Collection: {COLLECTION_NAME}...\")\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config={\n",
    "            \"dense\": models.VectorParams(\n",
    "                distance=models.Distance.COSINE,\n",
    "                size=1536, # <--- Ë´ãÁ¢∫Ë™çÈÄôË£°ÊòØÂê¶Á¨¶Âêà‰Ω†ÁöÑ Embedding Ê®°ÂûãÁ∂≠Â∫¶ÔºÅ\n",
    "            ),\n",
    "        },\n",
    "        # ÂïüÁî®Ê∑∑ÂêàÊ™¢Á¥¢ (Hybrid Search) ÁöÑÁ®ÄÁñèÂêëÈáèÈÖçÁΩÆ\n",
    "        sparse_vectors_config={\n",
    "            \"sparse\": models.SparseVectorParams(modifier=models.Modifier.IDF)\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"üìä Ê∫ñÂÇôËôïÁêÜ {len(documents)} ÂÄã Chunks...\")\n",
    "\n",
    "    # 3. Ê∫ñÂÇôÁ¥îÊñáÂ≠óÂàóË°®‰ª•ÈÄ≤Ë°å Embedding\n",
    "    # document Áâ©‰ª∂‰∏çËÉΩÁõ¥Êé• embedÔºåË¶ÅÂèñÂá∫ page_content\n",
    "    texts_to_embed = [doc.page_content for doc in documents]\n",
    "\n",
    "    print(\"‚è≥ Ë®àÁÆó Embeddings (Dense Vectors)...\")\n",
    "    try:\n",
    "        doc_embeddings = embedding_model.embed_documents(texts_to_embed)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Embedding Ë®àÁÆóÂ§±Êïó: {e}\")\n",
    "        return\n",
    "\n",
    "    if len(doc_embeddings) != len(documents):\n",
    "        print(\"‚ùå Embedding Êï∏ÈáèËàáÊñá‰ª∂Êï∏Èáè‰∏çÁ¨¶ÔºÅ\")\n",
    "        return\n",
    "\n",
    "    # 4. Ê∫ñÂÇôÂØ´ÂÖ• Qdrant ÁöÑ Points\n",
    "    points = []\n",
    "    print(\"‚è≥ Ê≠£Âú®Â∞ÅË£ùË≥áÊñô...\")\n",
    "\n",
    "    for doc, embedding in zip(documents, doc_embeddings):\n",
    "        # Áî¢Áîü UUID\n",
    "        point_id = uuid.uuid4().hex\n",
    "\n",
    "        # Âª∫Á´ã Payload (ÂåÖÂê´ÊñáÂ≠óËàá‰æÜÊ∫ê metadata)\n",
    "        payload = {\n",
    "            \"text\": doc.page_content,\n",
    "            \"source\": doc.metadata.get(\"source\", \"unknown\"), # Êää‰æÜÊ∫êÊ™îÂêçÂ≠òÈÄ≤Âéª\n",
    "            \"page\": doc.metadata.get(\"page\", 0) # Â¶ÇÊûúÊúâÁöÑË©±\n",
    "        }\n",
    "\n",
    "        # Âª∫Á´ã Point\n",
    "        point = models.PointStruct(\n",
    "            id=point_id,\n",
    "            vector={\n",
    "                \"dense\": embedding,\n",
    "                # ÈÄôË£°‰ΩøÁî® Qdrant ÁöÑÂêÑÁ®Æ BM25 ÊîØÊè¥ÊñπÂºèÔºåÂÇ≥ÂÖ•Á¥îÊñáÂ≠ó\n",
    "                \"sparse\": models.Document(text=doc.page_content, model=\"Qdrant/bm25\"),\n",
    "            },\n",
    "            payload=payload,\n",
    "        )\n",
    "        points.append(point)\n",
    "\n",
    "    # 5. ÊâπÊ¨°ÂØ´ÂÖ• (Upsert)\n",
    "    batch_size = 50\n",
    "    print(f\"‚è≥ ÈñãÂßãÂØ´ÂÖ• {len(points)} Á≠ÜË≥áÊñôËá≥ Qdrant...\")\n",
    "\n",
    "    for i in tqdm(range(0, len(points), batch_size), desc=\"Upserting\"):\n",
    "        batch_points = points[i : i + batch_size]\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=batch_points\n",
    "        )\n",
    "\n",
    "    print(f\"‚úÖ Ë≥áÊñôÂØ´ÂÖ•ÂÆåÊàêÔºÅCollection: {COLLECTION_NAME}\")"
   ],
   "id": "1e76a4e3437e6773",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T05:17:04.926092Z",
     "start_time": "2026-02-11T05:17:00.757396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"* ËºâÂÖ• Reranker Ê®°Âûã...\")\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    RERANKER_MODEL_PATH, local_files_only=True, trust_remote_code=True\n",
    ")\n",
    "reranker_model = AutoModelForCausalLM.from_pretrained(\n",
    "    RERANKER_MODEL_PATH, local_files_only=True, trust_remote_code=True\n",
    ").to(device_obj).eval()\n",
    "\n",
    "token_false_id = reranker_tokenizer.convert_tokens_to_ids(\"no\")\n",
    "token_true_id = reranker_tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "max_reranker_length = 8192\n",
    "\n",
    "prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "prefix_tokens = reranker_tokenizer.encode(prefix, add_special_tokens=False)\n",
    "suffix_tokens = reranker_tokenizer.encode(suffix, add_special_tokens=False)"
   ],
   "id": "d85b9bb1e5f3564c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* ËºâÂÖ• Reranker Ê®°Âûã...\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T05:17:05.009896Z",
     "start_time": "2026-02-11T05:17:04.975632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_rerank_scores(pairs, batch_size=4):\n",
    "    \"\"\"\n",
    "    ÂàÜÊâπË®àÁÆó Reranker ÂàÜÊï∏ÔºåÈÅøÂÖç MPS Out Of Memory\n",
    "    batch_size: Âª∫Ë≠∞Ë®≠Â∞è‰∏ÄÈªû (‰æãÂ¶Ç 2 Êàñ 4)\n",
    "    \"\"\"\n",
    "    all_scores = []\n",
    "\n",
    "    # ‰ΩøÁî® tqdm È°ØÁ§∫ Rerank ÈÄ≤Â∫¶ (ÂèØÈÅ∏)\n",
    "    # for i in range(0, len(pairs), batch_size):\n",
    "\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        batch_pairs = pairs[i: i + batch_size]\n",
    "\n",
    "        processed_inputs = []\n",
    "        for pair in batch_pairs:\n",
    "            pair_ids = reranker_tokenizer.encode(\n",
    "                pair, add_special_tokens=False, truncation=True,\n",
    "                max_length=max_reranker_length - len(prefix_tokens) - len(suffix_tokens)\n",
    "            )\n",
    "            full_ids = prefix_tokens + pair_ids + suffix_tokens\n",
    "            processed_inputs.append(reranker_tokenizer.decode(full_ids))\n",
    "\n",
    "        inputs = reranker_tokenizer(\n",
    "            processed_inputs, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_reranker_length\n",
    "        )\n",
    "\n",
    "        # ÁßªÂãïÂà∞ GPU\n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].to(device_obj)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = reranker_model(**inputs).logits[:, -1, :]\n",
    "            scores = logits[:, token_true_id].exp().tolist()\n",
    "            all_scores.extend(scores)\n",
    "\n",
    "        # Ê∏ÖÁêÜ GPU Ë®òÊÜ∂È´î\n",
    "        del inputs, logits, scores\n",
    "        if device_obj.type == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "        elif device_obj.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return all_scores"
   ],
   "id": "afe6079bc2578181",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T05:17:05.051012Z",
     "start_time": "2026-02-11T05:17:05.029889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rerank_documents(query, documents):\n",
    "    if not documents: return []\n",
    "\n",
    "    formatted_pairs = [\n",
    "        f\"<Instruct>: Ê†πÊìöÊü•Ë©¢Ê™¢Á¥¢Áõ∏ÈóúÊñá‰ª∂\\n<Query>: {query}\\n<Document>: {doc}\"\n",
    "        for doc in documents\n",
    "    ]\n",
    "\n",
    "    # ÈÄôË£°ÂëºÂè´‰øÆÊîπÂæåÁöÑÂáΩÊï∏Ôºåbatch_size Ë®≠ÁÇ∫ 4 (Ëã•ÈÇÑÊòØÁàÜË®òÊÜ∂È´îÔºåË´ãÊîπÊàê 2 Êàñ 1)\n",
    "    scores = compute_rerank_scores(formatted_pairs, batch_size=4)\n",
    "\n",
    "    doc_scores = list(zip(documents, scores))\n",
    "    doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return doc_scores"
   ],
   "id": "da6d82e39f745e54",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T05:17:05.059904Z",
     "start_time": "2026-02-11T05:17:05.052125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def query_rewrite(query: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    ‰Ω†ÊòØ‰∏ÄÂÄãÊêúÂ∞ãÂºïÊìéÂÑ™ÂåñÂ∞àÂÆ∂„ÄÇË´ãÂ∞á‰ª•‰∏ã‰ΩøÁî®ËÄÖÁöÑÂïèÈ°åÊîπÂØ´ÁÇ∫Êõ¥Á≤æÁ¢∫„ÄÅÈÅ©ÂêàÂÅöË™ûÁæ©Ê™¢Á¥¢ÁöÑÈóúÈçµÂ≠óÊü•Ë©¢„ÄÇ\n",
    "    ‰øùÁïôÊ†∏ÂøÉÊÑèÂúñÔºåÂéªÈô§Ë¥ÖË©ûÔºå‰∏¶ÈáùÂ∞çËá™‰æÜÊ∞¥ÂÖ¨Âè∏Áõ∏ÈóúÊ•≠ÂãôÈÄ≤Ë°åÂÑ™Âåñ„ÄÇ\n",
    "    Âè™Ëº∏Âá∫ÊîπÂØ´ÂæåÁöÑÂè•Â≠êÔºå‰∏çË¶ÅÊúâ‰ªª‰ΩïËß£Èáã„ÄÇ\n",
    "\n",
    "    ‰ΩøÁî®ËÄÖÂïèÈ°å: {query}\n",
    "    ÊîπÂØ´ÂæåÊü•Ë©¢:\n",
    "    \"\"\"\n",
    "    rewritten = llm_client.generate(prompt).strip()\n",
    "    return rewritten"
   ],
   "id": "14acbcc257d3f60",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def hybrid_search_with_rerank(query: str, initial_limit=20, final_limit=3):\n",
    "    query_vec = embedding_model.embed_query(query)\n",
    "\n",
    "    try:\n",
    "        response = client.query_points(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            prefetch=[\n",
    "                models.Prefetch(\n",
    "                    query=models.Document(text=query, model=\"Qdrant/bm25\"),\n",
    "                    using=\"sparse\",\n",
    "                    limit=initial_limit,\n",
    "                ),\n",
    "                models.Prefetch(\n",
    "                    query=query_vec,\n",
    "                    using=\"dense\",\n",
    "                    limit=initial_limit,\n",
    "                ),\n",
    "            ],\n",
    "            query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "            limit=initial_limit,\n",
    "        )\n",
    "        candidate_docs = [point.payload[\"text\"] for point in response.points]\n",
    "    except Exception as e:\n",
    "        print(f\"Search Error: {e}\")\n",
    "        return []\n",
    "\n",
    "    if not candidate_docs:\n",
    "        return []\n",
    "\n",
    "    # ÈÄôË£°ÈÄ≤Ë°å Rerank\n",
    "    top_results = rerank_documents(query, candidate_docs)[:final_limit]\n",
    "    return top_results"
   ],
   "id": "2d5d6491d8e053ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T05:17:05.090275Z",
     "start_time": "2026-02-11T05:17:05.071535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    print(f\"üìÇ ËÆÄÂèñ Excel: {PREDICT_INPUT}\")\n",
    "    if not os.path.exists(PREDICT_INPUT):\n",
    "        print(\"‚ùå Ê™îÊ°à‰∏çÂ≠òÂú®\")\n",
    "        return\n",
    "    df =  pd.read_csv(PREDICT_INPUT)\n",
    "\n",
    "    if 'q_id' not in df.columns: df['q_id'] = None\n",
    "    if 'answer' not in df.columns: df['answer'] = None\n",
    "\n",
    "    df['q_id'] = df['q_id'].astype('object')\n",
    "    df['answer'] = df['answer'].astype('object')\n",
    "\n",
    "    # „ÄêÊñ∞Â¢û„ÄëÁî®‰æÜÊö´Â≠ò Ground Truth (Context) ÁöÑÂàóË°®\n",
    "    ground_truth_list = []\n",
    "\n",
    "    print(\"üöÄ ÈñãÂßãËôïÁêÜÂïèÈ°å...\")\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        original_question = str(row['questions'])\n",
    "\n",
    "        current_uuid = str(uuid.uuid4())\n",
    "        refined_query = query_rewrite(original_question)\n",
    "        search_results = hybrid_search_with_rerank(refined_query)\n",
    "        retrieval_context = [doc for doc, score in search_results]\n",
    "        context_str = \"\\n\".join(retrieval_context)\n",
    "\n",
    "        ground_truth_list.append({\n",
    "                    \"id\": current_uuid,\n",
    "                    \"questions\": original_question,\n",
    "                    \"contexts\": retrieval_context,  # DeepEval ÈúÄË¶Å list\n",
    "                    \"ground_truth\": \"\"  # È†êÁïôÊ¨Ñ‰ΩçÔºåDeepEval ÁöÑ Recall ÈúÄË¶ÅÈÄôÂÄã (Ê®ôÊ∫ñÁ≠îÊ°à)\n",
    "                })\n",
    "\n",
    "        qa_prompt = f\"\"\"\n",
    "        ‰Ω†ÊòØ‰∏ÄÂÄãÂ∞àÊ•≠ÁöÑÂ∑•Âª†Ë≥áË®äÈÉ®Âä©Êâã„ÄÇË´ãÊ†πÊìö„ÄêÂèÉËÄÉË≥áÊñô„ÄëÂõûÁ≠î‰ΩøÁî®ËÄÖÁöÑ„ÄêÂïèÈ°å„Äë„ÄÇ\n",
    "\n",
    "        Ë¶èÁØÑÔºö\n",
    "        1. Á≠îÊ°àÂøÖÈ†àÂü∫ÊñºÂèÉËÄÉË≥áÊñôÔºå‰∏çË¶ÅÁ∑®ÈÄ†„ÄÇ\n",
    "        2. Â¶ÇÊûúÂèÉËÄÉË≥áÊñô‰∏çË∂≥‰ª•ÂõûÁ≠îÔºåË´ãÂõûÁ≠î„ÄåÁõÆÂâçË≥áË®ä‰∏çË∂≥ÔºåÂª∫Ë≠∞ËÅØÁπ´ÂÆ¢Êúç„Äç„ÄÇ\n",
    "        3. Ë™ûÊ∞£Ë¶™Âàá„ÄÅÂ∞àÊ•≠„ÄÇ\n",
    "\n",
    "        „ÄêÂèÉËÄÉË≥áÊñô„ÄëÔºö\n",
    "        {context_str}\n",
    "\n",
    "        „ÄêÂïèÈ°å„ÄëÔºö{original_question}\n",
    "\n",
    "        „ÄêÂõûÁ≠î„ÄëÔºö\n",
    "        \"\"\"\n",
    "        answer = llm_client.generate(qa_prompt)\n",
    "\n",
    "        # 4. Â∞á Answer ÂØ´ÂõûÂéüÊú¨ÁöÑ DataFrame\n",
    "        df.at[index, 'id'] = current_uuid\n",
    "        df.at[index, 'answer'] = answer\n",
    "\n",
    "    df.to_excel(PREDICT_OUPUT, index=False)\n",
    "    print(f\"‚úÖ Answer ËôïÁêÜÂÆåÊàêÔºÅÁµêÊûúÂ∑≤ÂÑ≤Â≠òËá≥: {PREDICT_OUPUT}\")\n",
    "\n",
    "    # Ê™îÊ°à 2ÔºöÂ≠ò Ground Truth (Context) ÁöÑ CSV\n",
    "    gt_df = pd.DataFrame(ground_truth_list)\n",
    "\n",
    "    # Â≠òÊàê CSV (Âª∫Ë≠∞Áî® utf-8-sig ‰ª•ÂÖç‰∏≠Êñá‰∫ÇÁ¢º)\n",
    "    gt_df.to_csv(\"ground_truth.csv\", index=False, encoding='utf-8-sig')\n",
    "    print(f\"‚úÖ Ground Truth (Context) Â∑≤ÂÑ≤Â≠òËá≥: ground_truth.csv\")"
   ],
   "id": "c5fa49bcec6e203",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T05:26:00.432236Z",
     "start_time": "2026-02-11T05:18:43.230032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "68e7788fd8048589",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[33], line 50\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     28\u001B[0m ground_truth_list\u001B[38;5;241m.\u001B[39mappend({\n\u001B[1;32m     29\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m: current_uuid,\n\u001B[1;32m     30\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquestions\u001B[39m\u001B[38;5;124m\"\u001B[39m: original_question,\n\u001B[1;32m     31\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontexts\u001B[39m\u001B[38;5;124m\"\u001B[39m: retrieval_context,  \u001B[38;5;66;03m# DeepEval ÈúÄË¶Å list\u001B[39;00m\n\u001B[1;32m     32\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mground_truth\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# È†êÁïôÊ¨Ñ‰ΩçÔºåDeepEval ÁöÑ Recall ÈúÄË¶ÅÈÄôÂÄã (Ê®ôÊ∫ñÁ≠îÊ°à)\u001B[39;00m\n\u001B[1;32m     33\u001B[0m         })\n\u001B[1;32m     35\u001B[0m qa_prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;124m‰Ω†ÊòØ‰∏ÄÂÄãÂ∞àÊ•≠ÁöÑÂ∑•Âª†Ë≥áË®äÈÉ®Âä©Êâã„ÄÇË´ãÊ†πÊìö„ÄêÂèÉËÄÉË≥áÊñô„ÄëÂõûÁ≠î‰ΩøÁî®ËÄÖÁöÑ„ÄêÂïèÈ°å„Äë„ÄÇ\u001B[39m\n\u001B[1;32m     37\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;124m„ÄêÂõûÁ≠î„ÄëÔºö\u001B[39m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124m\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m---> 50\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[43mllm_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mqa_prompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;66;03m# 4. Â∞á Answer ÂØ´ÂõûÂéüÊú¨ÁöÑ DataFrame\u001B[39;00m\n\u001B[1;32m     53\u001B[0m df\u001B[38;5;241m.\u001B[39mat[index, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m current_uuid\n",
      "Cell \u001B[0;32mIn[25], line 8\u001B[0m, in \u001B[0;36mSimpleLLMClient.generate\u001B[0;34m(self, prompt)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgenerate\u001B[39m(\u001B[38;5;28mself\u001B[39m, prompt: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m----> 8\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompletions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m}\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39mchoices[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mmessage\u001B[38;5;241m.\u001B[39mcontent\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/openai/_utils/_utils.py:286\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    284\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 286\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:1192\u001B[0m, in \u001B[0;36mCompletions.create\u001B[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m   1146\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m   1147\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1189\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m not_given,\n\u001B[1;32m   1190\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[1;32m   1191\u001B[0m     validate_response_format(response_format)\n\u001B[0;32m-> 1192\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1193\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1194\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m   1196\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1197\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1198\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maudio\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1199\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1200\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1201\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1202\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1203\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1204\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_completion_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1205\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1206\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1207\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodalities\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodalities\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1208\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1209\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparallel_tool_calls\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1210\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprediction\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1211\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1212\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprompt_cache_key\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt_cache_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1213\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprompt_cache_retention\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt_cache_retention\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1214\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreasoning_effort\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreasoning_effort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1215\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1216\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msafety_identifier\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43msafety_identifier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1217\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1218\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mservice_tier\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1219\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1220\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1221\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1222\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1223\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1224\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1225\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1226\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1227\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1228\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1229\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mverbosity\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbosity\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1230\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweb_search_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mweb_search_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1231\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1232\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParamsStreaming\u001B[49m\n\u001B[1;32m   1233\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\n\u001B[1;32m   1234\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParamsNonStreaming\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1235\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1236\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1237\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[1;32m   1238\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1239\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1240\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1241\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1242\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/openai/_base_client.py:1294\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1285\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1286\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1287\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease pass raw bytes via the `content` parameter instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1288\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[1;32m   1289\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m   1290\u001B[0m     )\n\u001B[1;32m   1291\u001B[0m opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1292\u001B[0m     method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, content\u001B[38;5;241m=\u001B[39mcontent, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1293\u001B[0m )\n\u001B[0;32m-> 1294\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/openai/_base_client.py:1002\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1000\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1001\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1002\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1003\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1004\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_should_stream_response_body\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1005\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1006\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1007\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mTimeoutException \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m   1008\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered httpx.TimeoutException\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/httpx/_client.py:914\u001B[0m, in \u001B[0;36mClient.send\u001B[0;34m(self, request, stream, auth, follow_redirects)\u001B[0m\n\u001B[1;32m    910\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_timeout(request)\n\u001B[1;32m    912\u001B[0m auth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_request_auth(request, auth)\n\u001B[0;32m--> 914\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_auth\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    915\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    916\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    917\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    918\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    919\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    920\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    921\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/httpx/_client.py:942\u001B[0m, in \u001B[0;36mClient._send_handling_auth\u001B[0;34m(self, request, auth, follow_redirects, history)\u001B[0m\n\u001B[1;32m    939\u001B[0m request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(auth_flow)\n\u001B[1;32m    941\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 942\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_redirects\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    943\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    944\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    945\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    946\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    947\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    948\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/httpx/_client.py:979\u001B[0m, in \u001B[0;36mClient._send_handling_redirects\u001B[0;34m(self, request, follow_redirects, history)\u001B[0m\n\u001B[1;32m    976\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_hooks[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequest\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m    977\u001B[0m     hook(request)\n\u001B[0;32m--> 979\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_single_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    981\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_hooks[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/httpx/_client.py:1014\u001B[0m, in \u001B[0;36mClient._send_single_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m   1009\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1010\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAttempted to send an async request with a sync Client instance.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1011\u001B[0m     )\n\u001B[1;32m   1013\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39mrequest):\n\u001B[0;32m-> 1014\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mtransport\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, SyncByteStream)\n\u001B[1;32m   1018\u001B[0m response\u001B[38;5;241m.\u001B[39mrequest \u001B[38;5;241m=\u001B[39m request\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/httpx/_transports/default.py:250\u001B[0m, in \u001B[0;36mHTTPTransport.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    237\u001B[0m req \u001B[38;5;241m=\u001B[39m httpcore\u001B[38;5;241m.\u001B[39mRequest(\n\u001B[1;32m    238\u001B[0m     method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[1;32m    239\u001B[0m     url\u001B[38;5;241m=\u001B[39mhttpcore\u001B[38;5;241m.\u001B[39mURL(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    247\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[1;32m    248\u001B[0m )\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[0;32m--> 250\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_pool\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp\u001B[38;5;241m.\u001B[39mstream, typing\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Response(\n\u001B[1;32m    255\u001B[0m     status_code\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mstatus,\n\u001B[1;32m    256\u001B[0m     headers\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[1;32m    257\u001B[0m     stream\u001B[38;5;241m=\u001B[39mResponseStream(resp\u001B[38;5;241m.\u001B[39mstream),\n\u001B[1;32m    258\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[1;32m    259\u001B[0m )\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    253\u001B[0m         closing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assign_requests_to_connections()\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_connections(closing)\n\u001B[0;32m--> 256\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    258\u001B[0m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[1;32m    259\u001B[0m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n\u001B[1;32m    260\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, typing\u001B[38;5;241m.\u001B[39mIterable)\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    232\u001B[0m connection \u001B[38;5;241m=\u001B[39m pool_request\u001B[38;5;241m.\u001B[39mwait_for_connection(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[0;32m--> 236\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    237\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpool_request\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[1;32m    240\u001B[0m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[1;32m    242\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n\u001B[1;32m    244\u001B[0m     pool_request\u001B[38;5;241m.\u001B[39mclear_connection()\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001B[0m, in \u001B[0;36mHTTPConnection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connect_failed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[0;32m--> 103\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_connection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/httpcore/_sync/http11.py:136\u001B[0m, in \u001B[0;36mHTTP11Connection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_closed\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[1;32m    135\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_closed()\n\u001B[0;32m--> 136\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/httpcore/_sync/http11.py:106\u001B[0m, in \u001B[0;36mHTTP11Connection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreceive_response_headers\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request, kwargs\n\u001B[1;32m     99\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[1;32m    100\u001B[0m     (\n\u001B[1;32m    101\u001B[0m         http_version,\n\u001B[1;32m    102\u001B[0m         status,\n\u001B[1;32m    103\u001B[0m         reason_phrase,\n\u001B[1;32m    104\u001B[0m         headers,\n\u001B[1;32m    105\u001B[0m         trailing_data,\n\u001B[0;32m--> 106\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_receive_response_headers\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m     trace\u001B[38;5;241m.\u001B[39mreturn_value \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    108\u001B[0m         http_version,\n\u001B[1;32m    109\u001B[0m         status,\n\u001B[1;32m    110\u001B[0m         reason_phrase,\n\u001B[1;32m    111\u001B[0m         headers,\n\u001B[1;32m    112\u001B[0m     )\n\u001B[1;32m    114\u001B[0m network_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_network_stream\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/httpcore/_sync/http11.py:177\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_response_headers\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    174\u001B[0m timeout \u001B[38;5;241m=\u001B[39m timeouts\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 177\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_receive_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11\u001B[38;5;241m.\u001B[39mResponse):\n\u001B[1;32m    179\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/httpcore/_sync/http11.py:217\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_event\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    214\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mnext_event()\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11\u001B[38;5;241m.\u001B[39mNEED_DATA:\n\u001B[0;32m--> 217\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_network_stream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mREAD_NUM_BYTES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    221\u001B[0m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[1;32m    222\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mtheir_state \u001B[38;5;241m==\u001B[39m h11\u001B[38;5;241m.\u001B[39mSEND_RESPONSE:\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/site-packages/httpcore/_backends/sync.py:128\u001B[0m, in \u001B[0;36mSyncStream.read\u001B[0;34m(self, max_bytes, timeout)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39msettimeout(timeout)\n\u001B[0;32m--> 128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/ssl.py:1292\u001B[0m, in \u001B[0;36mSSLSocket.recv\u001B[0;34m(self, buflen, flags)\u001B[0m\n\u001B[1;32m   1288\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1289\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1290\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1291\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1292\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuflen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1293\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv(buflen, flags)\n",
      "File \u001B[0;32m~/.mamba/envs/ai_for_course_py310/lib/python3.10/ssl.py:1165\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1163\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m, buffer)\n\u001B[1;32m   1164\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1165\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1166\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m SSLError \u001B[38;5;28;01mas\u001B[39;00m x:\n\u001B[1;32m   1167\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m SSL_ERROR_EOF \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msuppress_ragged_eofs:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T06:58:14.664199Z",
     "start_time": "2026-02-11T06:55:38.281926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pdfplumber\n",
    "from llm_guard.input_scanners import PromptInjection\n",
    "from llm_guard.input_scanners.prompt_injection import MatchType\n",
    "from docx import Document\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import requests\n",
    "import base64  # Êñ∞Â¢ûÔºöÁî®ÊñºÂúñÁâáËΩâÁ¢º\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient, models\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.documents import Document as LCDocument\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from io import BytesIO\n",
    "\n",
    "# ================= ÈÖçÁΩÆË®≠ÂÆö =================\n",
    "FILE_PATH = \"HW\"\n",
    "FILE_LIST = [\"1.pdf\", \"2.pdf\", \"3.pdf\", \"4.png\", \"5.docx\"]\n",
    "EMBEDDING_API_URL = \"http://ws-04.wade0426.me/embed\"\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "COLLECTION_NAME = \"rag_homework_day7_api\"\n",
    "\n",
    "# ‰∏ªË¶Å LLM (ÂõûÁ≠îÂïèÈ°åÁî®)\n",
    "LLM_BASE_URL = \"https://ws-03.wade0426.me/v1\"\n",
    "LLM_API_KEY = \"day7hw\"\n",
    "LLM_MODEL_NAME = \"/models/Qwen3-30B-A3B-Instruct-2507-FP8\"\n",
    "\n",
    "# OLM OCR Ê®°Âûã (ËôïÁêÜÂúñÁâáÁî®)\n",
    "OLM_API_URL = \"https://6c1f-163-17-132-191.ngrok-free.app/v1/chat/completions\"\n",
    "OLM_API_KEY = \"day7hw\"  # ÂÅáË®≠ Key Ëàá LLM Áõ∏ÂêåÔºåËã•‰∏çÂêåË´ã‰øÆÊîπ\n",
    "OLM_MODEL_NAME = \"allenai/olmOCR-2-7B-1025-FP8\"\n",
    "\n",
    "RERANKER_MODEL_PATH = os.path.expanduser(\"../day6/Qwen3-Reranker-0.6B\")\n",
    "PREDICT_INPUT = \"HW/questions.csv\"\n",
    "PREDICT_OUTPUT = \"HW/output.csv\"\n",
    "GROUND_TRUTH_OUTPUT = \"HW/ground_truth.csv\"\n",
    "\n",
    "client = QdrantClient(url=QDRANT_URL)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_obj = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_obj = torch.device(\"mps\")\n",
    "else:\n",
    "    device_obj = torch.device(\"cpu\")\n",
    "print(f\"* Device: {device_obj}\")\n",
    "\n",
    "\n",
    "# ================= È°ûÂà•ËàáÂáΩÂºèÂÆöÁæ© =================\n",
    "\n",
    "class CustomAPIEmbeddings(Embeddings):\n",
    "    def __init__(self, api_url):\n",
    "        self.api_url = api_url\n",
    "\n",
    "    def _call_api(self, texts: List[str]) -> List[List[float]]:\n",
    "        data = {\"texts\": texts, \"normalize\": True, \"batch_size\": 32}\n",
    "        try:\n",
    "            response = requests.post(self.api_url, json=data, timeout=60)\n",
    "            if response.status_code == 200:\n",
    "                return response.json().get('embeddings', [])\n",
    "            else:\n",
    "                print(f\"‚ùå API Error Code: {response.status_code}\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå API Exception: {e}\")\n",
    "            return []\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self._call_api(texts)\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        results = self._call_api([text])\n",
    "        return results[0] if results else []\n",
    "\n",
    "\n",
    "class SimpleLLMClient:\n",
    "    def __init__(self, base_url, model_name, api_key):\n",
    "        self.client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.3,\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"LLM Error: {e}\")\n",
    "            return \"Error generating response.\"\n",
    "\n",
    "\n",
    "# ÂàùÂßãÂåñ\n",
    "print(f\"* Initial Embedding API ({EMBEDDING_API_URL})...\")\n",
    "embedding_model = CustomAPIEmbeddings(EMBEDDING_API_URL)\n",
    "try:\n",
    "    test_vec = embedding_model.embed_query(\"Ê∏¨Ë©¶\")\n",
    "    if test_vec:\n",
    "        EMBED_DIM = len(test_vec)\n",
    "        print(f\"‚úÖ API ÈÄ£Á∑öÊàêÂäüÔºÅÂêëÈáèÁ∂≠Â∫¶: {EMBED_DIM}\")\n",
    "    else:\n",
    "        raise ValueError(\"API ÂõûÂÇ≥ÁÇ∫Á©∫\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå API Ê∏¨Ë©¶Â§±Êïó: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"* ÂàùÂßãÂåñ LLM Client...\")\n",
    "llm_client = SimpleLLMClient(LLM_BASE_URL, LLM_MODEL_NAME, LLM_API_KEY)\n",
    "\n",
    "\n",
    "# --- Êñ∞Â¢ûÔºöËôïÁêÜ OLM OCR ÁöÑÂäüËÉΩ ---\n",
    "\n",
    "def encode_image_to_base64(image_path, max_size=1024):\n",
    "    \"\"\"\n",
    "    Â∞áÂúñÁâáÁ∏ÆÂ∞èÂæåËΩâÊèõÁÇ∫ Base64 Â≠ó‰∏≤\n",
    "    max_size: Èï∑ÈÇäÁöÑÊúÄÂ§ßÂÉèÁ¥†ÂÄº (È†êË®≠ 1024ÔºåÂèØÊúâÊïàÂ∞á token Â£ìÂú® 8192 ‰ª•ÂÖß)\n",
    "    \"\"\"\n",
    "    with Image.open(image_path) as img:\n",
    "        # 1. Ê™¢Êü•‰∏¶Á∏ÆÂ∞èÂúñÁâá\n",
    "        if max(img.size) > max_size:\n",
    "            # Ë®àÁÆóÁ∏ÆÊîæÊØî‰æãÔºåÁ∂≠ÊåÅÈï∑ÂØ¨ÊØî\n",
    "            ratio = max_size / max(img.size)\n",
    "            new_size = (int(img.width * ratio), int(img.height * ratio))\n",
    "            img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "            print(f\"   (ÂúñÁâáÈÅéÂ§ßÔºåÂ∑≤Á∏ÆÂ∞èËá≥: {new_size})\")\n",
    "\n",
    "        # 2. ËΩâÁÇ∫ Bytes\n",
    "        buffered = BytesIO()\n",
    "        # ËΩâÁÇ∫ PNG (Êàñ JPEG ‰ª•Êõ¥ÁØÄÁúÅ tokenÔºå‰ΩÜ PNG Â∞çÊñáÂ≠óËºÉÊ∏ÖÊô∞)\n",
    "        img.save(buffered, format=\"PNG\")\n",
    "\n",
    "        return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "\n",
    "def call_olm_ocr_api(image_path):\n",
    "    \"\"\"ÂëºÂè´ OLM API ÈÄ≤Ë°åÂúñÁâáËæ®Ë≠ò\"\"\"\n",
    "    base64_image = encode_image_to_base64(image_path)\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {OLM_API_KEY}\"\n",
    "    }\n",
    "\n",
    "    # ÈÄôÊòØÊ®ôÊ∫ñÁöÑ Vision LLM Ë´ãÊ±ÇÊ†ºÂºè\n",
    "    payload = {\n",
    "        \"model\": OLM_MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Convert this image to markdown text. Preserve tables and formatting.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 4096,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(OLM_API_URL, headers=headers, json=payload, timeout=120)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            # ÂèñÂæóÁîüÊàêÁöÑÊñáÂ≠ó\n",
    "            content = result['choices'][0]['message']['content']\n",
    "            return content\n",
    "        else:\n",
    "            print(f\"‚ùå OLM API Error: {response.status_code} - {response.text}\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå OLM Connection Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def process_idp_files(file_path, file_list):\n",
    "    \"\"\"ËÆÄÂèñ‰∏¶ËôïÁêÜÊâÄÊúâÊ™îÊ°à (Êï¥Âêà OLM OCR)\"\"\"\n",
    "    docs_content = {}\n",
    "    print(\"--- ÈñãÂßã IDP ËôïÁêÜ ---\")\n",
    "\n",
    "    for file_name in file_list:\n",
    "        full_path = os.path.join(file_path, file_name)\n",
    "        if not os.path.exists(full_path):\n",
    "            print(f\"* Êâæ‰∏çÂà∞Ê™îÊ°à: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Ê≠£Âú®ËôïÁêÜ: {file_name}...\")\n",
    "        extracted_text = \"\"\n",
    "\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            try:\n",
    "                with pdfplumber.open(full_path) as pdf:\n",
    "                    for page in pdf.pages:\n",
    "                        text = page.extract_text()\n",
    "                        if text: extracted_text += text + \"\\n\"\n",
    "                        tables = page.extract_tables()\n",
    "                        for table in tables:\n",
    "                            table_str = \"\"\n",
    "                            for row in table:\n",
    "                                cleaned_row = [str(cell) if cell is not None else \"\" for cell in row]\n",
    "                                table_str += \" | \".join(cleaned_row) + \"\\n\"\n",
    "                            extracted_text += \"\\n[Ë°®Ê†º]\\n\" + table_str + \"\\n\"\n",
    "                print(f\"‚úÖ [{file_name}] PDF ÊèêÂèñÊàêÂäü\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå PDF ÈåØË™§: {e}\")\n",
    "\n",
    "        # 3.„Äê‰øÆÊîπ„ÄëÈáùÂ∞ç PNG ‰ΩøÁî® OLM API\n",
    "        elif file_name.endswith(\".png\"):\n",
    "            try:\n",
    "                print(f\"‚è≥ Ê≠£Âú®ÂëºÂè´ OLM Ê®°ÂûãËôïÁêÜÂúñÁâá ({file_name})...\")\n",
    "                olm_text = call_olm_ocr_api(full_path)\n",
    "\n",
    "                if olm_text:\n",
    "                    extracted_text = olm_text\n",
    "                    print(f\"‚úÖ [{file_name}] OLM Ëæ®Ë≠òÊàêÂäü\")\n",
    "                    # print(f\"È†êË¶Ω: {olm_text[:100]}...\") # Èô§ÈåØÁî®\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è [{file_name}] OLM Êú™ÂõûÂÇ≥ÂÖßÂÆπ\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå OLM ËôïÁêÜÂ§±Êïó {file_name}: {e}\")\n",
    "\n",
    "        elif file_name.endswith(\".docx\"):\n",
    "            try:\n",
    "                doc = Document(full_path)\n",
    "                for para in doc.paragraphs:\n",
    "                    extracted_text += para.text + \"\\n\"\n",
    "                print(f\"‚úÖ [{file_name}] Word ÊàêÂäü\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Word ÈåØË™§: {e}\")\n",
    "\n",
    "        if extracted_text.strip():\n",
    "            docs_content[file_name] = extracted_text\n",
    "\n",
    "    return docs_content\n",
    "\n",
    "\n",
    "def scan_for_injection(docs_content):\n",
    "    \"\"\"ÂÆâÂÖ®ÊÄßÊéÉÊèè\"\"\"\n",
    "    print(\"--- Âü∑Ë°åÂÆâÂÖ®ÊÄßÊéÉÊèè ---\")\n",
    "    scanner = PromptInjection(threshold=0.5, match_type=MatchType.FULL)\n",
    "    for filename, content in docs_content.items():\n",
    "        _, is_valid, score = scanner.scan(str(content))\n",
    "        if not is_valid:\n",
    "            print(f\"‚ö†Ô∏è Ë≠¶Âëä: [{filename}] ÁôºÁèæÊΩõÂú®ÊÉ°ÊÑèÊåá‰ª§ (Score: {score})\")\n",
    "        else:\n",
    "            print(f\"‚úÖ [{filename}] ÂÆâÂÖ®\")\n",
    "\n",
    "\n",
    "def init_qdrant_collection(documents):\n",
    "    \"\"\"ÂØ´ÂÖ•ÂêëÈáèË≥áÊñôÂ∫´\"\"\"\n",
    "    if not documents:\n",
    "        print(\"‚ö†Ô∏è ÁÑ°Êñá‰ª∂ÂèØÂØ´ÂÖ•\")\n",
    "        return\n",
    "\n",
    "    print(f\"üîÑ ÈáçÁΩÆÈõÜÂêà {COLLECTION_NAME}...\")\n",
    "    try:\n",
    "        client.delete_collection(COLLECTION_NAME)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config={\"dense\": models.VectorParams(distance=models.Distance.COSINE, size=EMBED_DIM)},\n",
    "        sparse_vectors_config={\"sparse\": models.SparseVectorParams(modifier=models.Modifier.IDF)},\n",
    "    )\n",
    "\n",
    "    texts_to_embed = [doc.page_content for doc in documents]\n",
    "    print(f\"‚è≥ Ë®àÁÆó {len(texts_to_embed)} Á≠Ü Embeddings...\")\n",
    "\n",
    "    doc_embeddings = embedding_model.embed_documents(texts_to_embed)\n",
    "    if len(doc_embeddings) != len(documents):\n",
    "        print(\"‚ùå Embedding Êï∏Èáè‰∏çÁ¨¶\")\n",
    "        return\n",
    "\n",
    "    points = []\n",
    "    for doc, embedding in zip(documents, doc_embeddings):\n",
    "        points.append(models.PointStruct(\n",
    "            id=uuid.uuid4().hex,\n",
    "            vector={\n",
    "                \"dense\": embedding,\n",
    "                \"sparse\": models.Document(text=doc.page_content, model=\"Qdrant/bm25\"),\n",
    "            },\n",
    "            payload={\n",
    "                \"text\": doc.page_content,\n",
    "                \"source\": doc.metadata.get(\"source\", \"unknown\")\n",
    "            },\n",
    "        ))\n",
    "\n",
    "    batch_size = 50\n",
    "    for i in tqdm(range(0, len(points), batch_size), desc=\"ÂØ´ÂÖ• Qdrant\"):\n",
    "        client.upsert(collection_name=COLLECTION_NAME, points=points[i: i + batch_size])\n",
    "    print(\"‚úÖ Ë≥áÊñôÂØ´ÂÖ•ÂÆåÊàê\")\n",
    "\n",
    "\n",
    "# --- Reranker Áõ∏Èóú ---\n",
    "print(\"* ËºâÂÖ• Reranker Ê®°Âûã...\")\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL_PATH, local_files_only=True, trust_remote_code=True)\n",
    "reranker_model = AutoModelForCausalLM.from_pretrained(RERANKER_MODEL_PATH, local_files_only=True,\n",
    "                                                      trust_remote_code=True).to(device_obj).eval()\n",
    "\n",
    "token_false_id = reranker_tokenizer.convert_tokens_to_ids(\"no\")\n",
    "token_true_id = reranker_tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "\n",
    "def compute_rerank_scores(pairs, batch_size=4):\n",
    "    all_scores = []\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        batch_pairs = pairs[i: i + batch_size]\n",
    "        processed_inputs = []\n",
    "        for pair in batch_pairs:\n",
    "            text = f\"{prefix}{pair}{suffix}\"\n",
    "            processed_inputs.append(text)\n",
    "\n",
    "        inputs = reranker_tokenizer(processed_inputs, padding=True, truncation=True, return_tensors=\"pt\",\n",
    "                                    max_length=1024).to(device_obj)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = reranker_model(**inputs).logits[:, -1, :]\n",
    "            scores = logits[:, token_true_id].exp().tolist()\n",
    "            all_scores.extend(scores)\n",
    "\n",
    "        del inputs, logits\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "    return all_scores\n",
    "\n",
    "\n",
    "def rerank_documents(query, documents):\n",
    "    if not documents: return []\n",
    "    formatted_pairs = [f\"<Instruct>: Ê†πÊìöÊü•Ë©¢Ê™¢Á¥¢Áõ∏ÈóúÊñá‰ª∂\\n<Query>: {query}\\n<Document>: {doc['text']}\" for doc in\n",
    "                       documents]\n",
    "    scores = compute_rerank_scores(formatted_pairs)\n",
    "    doc_scores = list(zip(documents, scores))\n",
    "    doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return doc_scores\n",
    "\n",
    "\n",
    "def hybrid_search_with_rerank(query: str, initial_limit=20, final_limit=3):\n",
    "    query_vec = embedding_model.embed_query(query)\n",
    "    try:\n",
    "        response = client.query_points(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            prefetch=[\n",
    "                models.Prefetch(query=models.Document(text=query, model=\"Qdrant/bm25\"), using=\"sparse\",\n",
    "                                limit=initial_limit),\n",
    "                models.Prefetch(query=query_vec, using=\"dense\", limit=initial_limit),\n",
    "            ],\n",
    "            query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "            limit=initial_limit,\n",
    "        )\n",
    "        candidate_docs = [{\"text\": point.payload.get(\"text\", \"\"), \"source\": point.payload.get(\"source\", \"unknown\")} for\n",
    "                          point in response.points]\n",
    "    except Exception as e:\n",
    "        print(f\"Search Error: {e}\")\n",
    "        return []\n",
    "\n",
    "    if not candidate_docs: return []\n",
    "    top_results = rerank_documents(query, candidate_docs)[:final_limit]\n",
    "    return top_results\n",
    "\n",
    "\n",
    "def query_rewrite(query: str) -> str:\n",
    "    prompt = f\"‰Ω†ÊòØ‰∏ÄÂÄãÊêúÂ∞ãÂºïÊìéÂÑ™ÂåñÂ∞àÂÆ∂„ÄÇË´ãÂ∞á‰ª•‰∏ã‰ΩøÁî®ËÄÖÁöÑÂïèÈ°åÊîπÂØ´ÁÇ∫Êõ¥Á≤æÁ¢∫ÁöÑÈóúÈçµÂ≠óÊü•Ë©¢„ÄÇ\\n‰ΩøÁî®ËÄÖÂïèÈ°å: {query}\\nÊîπÂØ´ÂæåÊü•Ë©¢:\"\n",
    "    return llm_client.generate(prompt).strip()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1. IDP ËôïÁêÜ (ÁèæÂú®‰ΩøÁî® OLM API)\n",
    "    docs_content = process_idp_files(FILE_PATH, FILE_LIST)\n",
    "\n",
    "    # 2. ÂÆâÂÖ®ÊÄßÊéÉÊèè\n",
    "    scan_for_injection(docs_content)\n",
    "\n",
    "    # 3. ÂàáÂàÜ\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"], chunk_size=500, chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    all_documents = []\n",
    "    for filename, text in docs_content.items():\n",
    "        all_documents.append(LCDocument(page_content=text, metadata={\"source\": filename}))\n",
    "\n",
    "    split_docs = text_splitter.split_documents(all_documents)\n",
    "    print(f\"üìä ÂÖ±ÂàáÂàÜÂá∫ {len(split_docs)} ÂÄãÂçÄÂ°ä\")\n",
    "\n",
    "    # 4. ÂØ´ÂÖ•ÂêëÈáèÂ∫´\n",
    "    init_qdrant_collection(split_docs)\n",
    "\n",
    "    print(f\"üìÇ ËÆÄÂèñÂïèÈ°å: {PREDICT_INPUT}\")\n",
    "    if not os.path.exists(PREDICT_INPUT):\n",
    "        print(\"‚ùå Ê™îÊ°à‰∏çÂ≠òÂú®\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(PREDICT_INPUT)\n",
    "\n",
    "    # Ê∏¨Ë©¶Ââç 5 È°å\n",
    "    df = df.head(5)\n",
    "    print(f\"‚ö†Ô∏è Ê∏¨Ë©¶Ê®°ÂºèÂïüÂãïÔºöÂÉÖËôïÁêÜÂâç {len(df)} È°åË≥áÊñô\")\n",
    "\n",
    "    if 'answer' not in df.columns: df['answer'] = None\n",
    "    if 'source' not in df.columns: df['source'] = None\n",
    "    if 'id' not in df.columns:\n",
    "        print(\"‚ùå Áº∫Â∞ë id Ê¨Ñ‰Ωç\")\n",
    "        return\n",
    "\n",
    "    ground_truth_list = []\n",
    "\n",
    "    print(\"üöÄ ÈñãÂßãÂõûÁ≠îÂïèÈ°å...\")\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        original_question = str(row['questions'])\n",
    "\n",
    "        refined_query = query_rewrite(original_question)\n",
    "        search_results = hybrid_search_with_rerank(refined_query)\n",
    "\n",
    "        if search_results:\n",
    "            retrieval_docs = [item[0] for item in search_results]\n",
    "            context_str = \"\\n\".join([doc['text'] for doc in retrieval_docs])\n",
    "            unique_sources = sorted(list(set([doc['source'] for doc in retrieval_docs])))\n",
    "            source_str = \",\".join(unique_sources)\n",
    "        else:\n",
    "            context_str = \"\"\n",
    "            source_str = \"\"\n",
    "            retrieval_docs = []\n",
    "\n",
    "        ground_truth_list.append({\n",
    "            \"id\": row['id'],\n",
    "            \"questions\": original_question,\n",
    "            \"contexts\": [doc['text'] for doc in retrieval_docs],\n",
    "            \"ground_truth\": \"\"\n",
    "        })\n",
    "\n",
    "        qa_prompt = f\"\"\"\n",
    "        ‰Ω†ÊòØ‰∏ÄÂÄãÂ∞àÊ•≠ÁöÑË≥áË®äÂä©Êâã„ÄÇË´ãÊ†πÊìö„ÄêÂèÉËÄÉË≥áÊñô„ÄëÂõûÁ≠îÂïèÈ°å„ÄÇ\n",
    "        Ëã•Ë≥áÊñô‰∏çË∂≥Ë´ãÂõûÁ≠î„ÄåÁõÆÂâçË≥áË®ä‰∏çË∂≥„Äç„ÄÇ\n",
    "        Ë´ãÁõ¥Êé•ÂõûÁ≠îÈáçÈªûÔºå‰∏çË¶ÅÈáçË§áÂïèÈ°å„ÄÇ\n",
    "\n",
    "        „ÄêÂèÉËÄÉË≥áÊñô„ÄëÔºö\n",
    "        {context_str}\n",
    "\n",
    "        „ÄêÂïèÈ°å„ÄëÔºö{original_question}\n",
    "        „ÄêÂõûÁ≠î„ÄëÔºö\n",
    "        \"\"\"\n",
    "        answer = llm_client.generate(qa_prompt)\n",
    "\n",
    "        df.at[index, 'answer'] = answer\n",
    "        df.at[index, 'source'] = source_str\n",
    "\n",
    "    df.to_csv(PREDICT_OUTPUT, index=False, encoding='utf-8-sig')\n",
    "    print(f\"‚úÖ ÁµêÊûúÂ∑≤ÂÑ≤Â≠ò: {PREDICT_OUTPUT}\")\n",
    "\n",
    "    pd.DataFrame(ground_truth_list).to_csv(GROUND_TRUTH_OUTPUT, index=False, encoding='utf-8-sig')\n",
    "    print(f\"‚úÖ GT Â∑≤ÂÑ≤Â≠ò: {GROUND_TRUTH_OUTPUT}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "26c065ed2c1eb6fb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
